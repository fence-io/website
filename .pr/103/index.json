[{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/ci/","section":"Tags","summary":"","title":"CI","type":"tags"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/","section":"Fence","summary":"","title":"Fence","type":"page"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/series/kubernetes-policy-engine/","section":"Series","summary":"","title":"Kubernetes Policy Engine","type":"series"},{"content":" Introduction # kubewarden is a Kubernetes-native policy engine designed to enforce custom security policies across Kubernetes clusters. This article will delve into the types of policies supported by Kubewarden, the syntax and language used to write these policies, its audit mode and reporting capabilities, observability features, offline evaluation in CI pipelines, and the extensive policies catalog.\nOverview and Purpose # Kubewarden integrates seamlessly into Kubernetes environments, offering a robust solution for implementing and enforcing security policies. It operates based on the principle of warden policies, which are written in WebAssembly (Wasm) format, allowing for lightweight, efficient execution directly within Kubernetes clusters.\nTypes of Policies # The policies are executed during the admission phase of the Kubernetes API server. They can accept, reject, or mutate incoming requests based on predefined rules to help ensuring that only compliant resources are created or modified within the cluster.\nKubewarden policies can be categorized into two main types:\nMutation Policies: Mutation policies modify resource definitions as they pass through the Kubernetes API server. These policies can be used to add annotations or labels, and ensure that resources adhere to organizational standards before they are persisted.\nValidation Policies: These policies validate resource definitions against specific criteria. Validation policies can reject non-compliant resources, helping maintain the desired state of the cluster by ensuring that only valid configurations are allowed.\nFor more information about the different types of policies, please refer to this link.\nSyntax and Language for Writing Policies # Kubewarden policies are written using the WebAssembly (Wasm) technology, which allows policies to be written in multiple programming languages that can compile to Wasm (list of supported languages). The most commonly used languages for writing Kubewarden policies are:\nRust: Known for its performance and safety, Rust is a popular choice for writing Kubewarden policies.\nGo: Go offers simplicity and readability, making it another viable option for policy authors.\nAssemblyScript: A TypeScript-like language that provides a more approachable syntax for JavaScript and TypeScript developers.\nThe flexibility of using WebAssembly means that policies can leverage existing libraries and tools within these languages, making it easier to implement complex logic and integrations.\nLeveraging Wasm, Kubewarden ensures that policies are executed securely and efficiently with minimal performance overhead. This flexibility allows organizations to address unique security requirements and compliance standards effectively.\nWasm modules are sandboxed and isolated.\nAudit Mode and Reporting # Kubewarden provides an audit mode that allows policies to be evaluated without enforcing them.\nThis mode is crucial for organizations looking to understand the impact of potential policies before applying them in a production environment.\nIn audit mode, policy evaluations generate detailed reports that highlight which resources would have been accepted, rejected, or mutated if the policies were enforced.\nThe results of the audit scanner are stored in a PolicyReport format, allowing integration with the Policy Reporter UI.\nObservability of the Solution # Observability is a key feature of Kubewarden, providing insights into the performance and effectiveness of policies. Kubewarden integrates with standard observability tools, offering:\nMetrics: Exported to Prometheus, these metrics provide visibility into policy evaluations, including the number of evaluations, decisions (accept/reject/mutate), and evaluation durations.\nLogs: Detailed logs are available for each policy evaluation, including the input data, evaluation results, and any errors encountered. These logs can be integrated with centralized logging solutions like Elasticsearch and Grafana.\nTracing: By integrating with distributed tracing systems, Kubewarden can provide end-to-end visibility into policy evaluations, helping diagnose performance issues and understand policy impacts.\nOffline Evaluation (Integration in CI Pipelines) # Kubewarden supports offline policy evaluation, making it an excellent fit for CI/CD pipelines. By integrating Kubewarden into CI pipelines, organizations can ensure that resource definitions are compliant with policies before they are deployed to a Kubernetes cluster. This is achieved by:\nPolicy Testing: Policies can be tested against resource definitions during the build phase, catching non-compliant configurations early in the development lifecycle. More details about policy testing are here.\nThe testing approach involves developers who create the policies, ensuring they receive immediate feedback on policy compliance in the CI pipeline. This allows them to address issues before merging code.\nAdditionally, cluster operators benefit from a feedback loop where resource definitions are evaluated against policies as part of the deployment pipeline, preventing non-compliant resources from reaching the cluster.\nPolicies Catalog # Kubewarden offers pre-defined policies that address common use cases and best practices. The policies catalog includes:\nSecurity Policies: Enforce security best practices, such as Pod Security Policy standard, and validating container images.\nCompliance Policies: Ensure resources adhere to regulatory requirements and organizational standards, such as enforcing or validating the presence of specific labels or annotations.\nOperational Policies: Improve cluster operations by enforcing naming conventions, validating configuration settings, and ensuring resource consistency.\nThis list serves as a starting point for organizations, providing ready-to-use policies that can be customized to meet specific needs.\nInstallation # Installing Kubewarden involves several steps to set up the necessary components for Kubernetes integration and policy enforcement. Here’s a general guide on how to install Kubewarden:\nRequirement: Install cert-manager before the kubewarden-controller chart.\nhelm repo add jetstack https://charts.jetstack.io helm install --wait --namespace cert-manager --create-namespace \\ --set installCRDs=true cert-manager jetstack/cert-manager helm repo add kubewarden https://charts.kubewarden.io helm repo update kubewarden helm install --wait -n kubewarden --create-namespace kubewarden-crds kubewarden/kubewarden-crds helm install --wait -n kubewarden kubewarden-controller kubewarden/kubewarden-controller helm install --wait -n kubewarden kubewarden-defaults kubewarden/kubewarden-defaults Example # Let\u0026rsquo;s create a very basic Kubewarden policy in Go. This policy will ensure that any Kubernetes resource has a specific label. (The policy created using go-policy-template).\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/kubewarden/policy-sdk-go/policy\u0026#34; \u0026#34;github.com/kubewarden/policy-sdk-go/protocol\u0026#34; ) type Settings struct { RequiredLabelKey string `json:\u0026#34;required_label_key\u0026#34;` RequiredLabelValue string `json:\u0026#34;required_label_value\u0026#34;` } func validate(payload []byte) ([]byte, error) { var req protocol.ValidationRequest if err := json.Unmarshal(payload, \u0026amp;req); err != nil { return nil, fmt.Errorf(\u0026#34;cannot unmarshal validation request: %w\u0026#34;, err) } var settings Settings if err := json.Unmarshal(req.Settings, \u0026amp;settings); err != nil { return nil, fmt.Errorf(\u0026#34;cannot unmarshal settings: %w\u0026#34;, err) } labels := req.Request.Object.Object[\u0026#34;metadata\u0026#34;].(map[string]interface{})[\u0026#34;labels\u0026#34;].(map[string]interface{}) if value, found := labels[settings.RequiredLabelKey]; !found || value != settings.RequiredLabelValue { return policy.Deny(fmt.Sprintf(\u0026#34;missing required label: %s=%s\u0026#34;, settings.RequiredLabelKey, settings.RequiredLabelValue)), nil } return policy.Allow(), nil } func main() { policy.Entrypoint(validate) } Build and Package the Policy # Install TinyGo: The official Go compiler can\u0026rsquo;t produce WebAssembly binaries that run outside the browser. Therefore, you must use TinyGo to build the policy.\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.25.0/tinygo_0.25.0_amd64.deb sudo dpkg -i tinygo_0.25.0_amd64.deb Compile the Policy:\nUse TinyGo to compile the policy into a WebAssembly module.\ntinygo build -o policy.wasm -target=wasi main.go Test the Policy # You can use wasmtime to test your Wasm module locally.\ncurl https://wasmtime.dev/install.sh -sSf | bash Create a settings.json file with the required label settings:\n{ \u0026#34;required_label_key\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;required_label_value\u0026#34;: \u0026#34;production\u0026#34; } Create a test-input.json file with a test Kubernetes resource:\n{ \u0026#34;request\u0026#34;: { \u0026#34;object\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;environment\u0026#34;: \u0026#34;production\u0026#34; } } } }, \u0026#34;settings\u0026#34;: { \u0026#34;required_label_key\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;required_label_value\u0026#34;: \u0026#34;production\u0026#34; } } Run the policy with Wasmtime and test input:\ncat test-input.json | wasmtime policy.wasm Deploy the Policy # Push the Policy to a Registry:\nYou need to push the compiled WebAssembly module to a registry that Kubewarden can access. Here, we will use cosign to sign and push the module to an OCI-compliant registry.\nInstall cosign:\ngo install github.com/sigstore/cosign/cmd/cosign@latest Push the WebAssembly module:\ncosign sign-blob --key cosign.key policy.wasm Deploy the Policy using Kubewarden:\nCreate a ClusterAdmissionPolicy resource to deploy your policy:\napiVersion: policies.kubewarden.io/v1 kind: ClusterAdmissionPolicy metadata: name: my-policy spec: module: registry://\u0026lt;your-registry\u0026gt;/policy:latest rules: - apiGroups: [\u0026#34;\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] mutating: false settings: required_label_key: \u0026#34;example-key\u0026#34; required_label_value: \u0026#34;example-value\u0026#34; Replace \u0026lt;your-registry\u0026gt; with the actual registry where you pushed your WebAssembly module.\nConclusion: # Kubewarden emerges as a valuable tool for Kubernetes administrators and security teams seeking to implement and enforce robust security policies effectively. Its use of WebAssembly for policy execution, comprehensive policy library, seamless Kubernetes integration, and active community support make it a compelling choice for enhancing Kubernetes security with policy-driven controls.\nHowever, adopting Kubewarden requires a learning curve for Wasm and Kubernetes concepts, and effective policy development may demand initial investment in understanding and implementation.\n","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/policy-engine/kubewarden-review/","section":"Articles","summary":"Introduction # kubewarden is a Kubernetes-native policy engine designed to enforce custom security policies across Kubernetes clusters. This article will delve into the types of policies supported by Kubewarden, the syntax and language used to write these policies, its audit mode and reporting capabilities, observability features, offline evaluation in CI pipelines, and the extensive policies catalog.\nOverview and Purpose # Kubewarden integrates seamlessly into Kubernetes environments, offering a robust solution for implementing and enforcing security policies.","title":"Kubernetes Policy Engine - kubewarden","type":"articles"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/policy/","section":"Tags","summary":"","title":"Policy","type":"tags"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/runtime/","section":"Tags","summary":"","title":"Runtime","type":"tags"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"June 14, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/wasm/","section":"Tags","summary":"","title":"Wasm","type":"tags"},{"content":"","date":"May 24, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/cni/","section":"Tags","summary":"","title":"CNI","type":"tags"},{"content":"","date":"May 24, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/container/","section":"Tags","summary":"","title":"Container","type":"tags"},{"content":"This article delves into the underlying processes involved in creating a pod and traces the path of network traffic within a Kubernetes cluster, from an initial web request to the container hosting the application. We will explore:\nWhat steps involved in the pod creation How containers communicate locally (Intra-Pod communication). Pod-to-Pod communication when the pods are on the same and different nodes. Pod-to-Service communication, where a pod sends traffic to another pod behind a service in Kubernetes. Requirements # Read Diving deep into Container Networking Docker KinD Kubectl Linux host system What Happens When Creating a Pod # A Pod is the smallest deployable unit in Kubernetes, encapsulating one or more containers. It utilizes two powerful Linux features, Linux namespaces and cgroups for isolation and resource management.\nLinux Namespaces # According to the man page:\n' A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource' Put simply, namespaces isolate processes, making them appear to have their own private instances, with changes visible only within the same namespace. In the Linux kernel, various types of namespaces exist, each with distinct properties.\nUser Namespace: Isolates user and group IDs. A process can have root privileges within its user namespace without having root privileges in other namespaces. PID Namespace: Provides an independent set of process IDs. The first process in a new namespace gets PID 1, similar to the init process. Network Namespace: Isolates the network stack, including routing tables, IP addresses, sockets, and firewall rules (as discussed previously here) Mount Namespace: Isolates the list of mount points. Filesystems can be mounted/unmounted within this namespace without affecting the host filesystem. IPC Namespace: Isolates inter-process communication resources like message queues and shared memory. UTS Namespace: Isolates hostname and domain name. Cgroups # Cgroups (control groups) limit the resources that a process can use, including CPU, memory, and I/O. In the context of containers, the container runtime continually monitors the resource usage of the container via cgroups.\nIf a container exceeds its limits, cgroups can enforce the constraints by throttling or killing the processes within the container.\nSteps in Pod Creation # Creating a pod in Kubernetes involves several detailed steps, from high-level orchestration down to low-level OS operations:\nThe API server authenticates the user, authorizes the request, validates the pod specification, and writes it to etcd. The scheduler then assigns the pod to a node, and the Kubelet on that node detects the new assignment and begins creating the pod.\nIsolation # The Kubelet interacts with the container runtime to create the containers defined in the pod. The container runtime pulls the required images and sets up namespaces for resource isolation.\nYou can visualize the Linux namespaces on a Kubernetes node by running lsns, which lists all namespaces created for containers on the host and provides the PIDs of the processes running in each namespace. To see the namespaces for a specific container, use its PID and run:\nroot@kind-control-plane:/# ls /proc/PID/ns cgroup\tipc mnt net pid pid_for_children time time_for_children user uts OR\nroot@kind-control-plane:/# lsns -p PID NS TYPE NPROCS PID USER COMMAND 4026531834 time 25 1 root /sbin/init 4026531837 user 25 1 root /sbin/init 4026532674 uts 25 1 root /sbin/init 4026532677 net 24 1 root /sbin/init 4026532812 ipc 2 336 65535 /pause 4026532820 mnt 1 439 root kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node 4026532821 pid 1 439 root kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node 4026532822 cgroup 1 439 root kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization-mode=Node Resource Management # When deploying a pod, you can specify resource limits and requests in the pod specification. Kubernetes communicates these specifications to the container runtime, which configures cgroups accordingly to enforce these resource constraints.\nCPU Limits: cgroups can limit the amount of CPU time a container can use. For example, you can specify that a container can only use 50% of a single CPU core:\necho \u0026#34;512\u0026#34; \u0026gt; /sys/fs/cgroup/cpu/my_container/cpu.shares Memory Limits: cgroups can limit the amount of memory a container can use. If a container tries to use more memory than allocated, it can be killed or throttled:\necho \u0026#34;500M\u0026#34; \u0026gt; /sys/fs/cgroup/memory/my_container/memory.limit_in_bytes I/O Limits: cgroups can limit the read and write speeds of devices associated with the container:\necho \u0026#34;8:16 1024\u0026#34; \u0026gt; /sys/fs/cgroup/blkio/my_container/blkio.throttle.read_bps_device Pause container ? # When checking the nodes of your Kubernetes cluster, you may notice some containers named \u0026ldquo;pause\u0026rdquo; when you run docker ps on the node. The pause container is a minimal container used as the parent for other containers in a Pod and serves several crucial roles.\nThe pause container is the first to start when a Pod is created, setting up the shared namespace environment for the other containers and then remaining running while consuming minimal resources.\nIt holds the network namespace and other shared namespaces (such as PID, IPC, and UTS) for the Pod. By maintaining these namespaces, the pause container ensures that all containers in the Pod can be restarted independently without losing the network setup or other shared resources.\nWithout the pause container, each time a container crashes inside the Pod, the CNI would need to reconfigure the network and go through all the steps previously listed, which would disrupt the service.\nAssigning IP to Pod # When a pod is assigned to a node,\nThe kubelet interacts with the container runtime through the CRI to create the container.\nAs part of the container creation process, the CRI runtime triggers the CNI plugin to set up networking for the container. The CNI plugin provid the necessary networking configuration, including the network namespace of the container.\nThe CNI plugin configures the network interface inside the container’s network namespace. The plugin allocates an IP address to the container. The plugin sets up routing rules and connects the container to the appropriate network (by configuring a bridge, VXLAN, etc.). The CRI runtime sets up DNS resolution inside the container. The pod registers with the cluster DNS. Once the CNI plugin has successfully set up networking, it returns the network configuration (e.g., the assigned IP address) to the CRI runtime. The CRI runtime passes this information back to the kubelet. The kubelet completes the pod creation process, and the pod becomes ready with its network fully configured.\nComparing CNIs # CNI plugins manage the creation, management, and deletion of network interfaces in containers, enabling communication both within the same host and across different hosts.\nThere are two main types of CNIs: flat networks and overlay networks.\nFlat Network CNI:\nConnects containers directly to a physical network, making them appear as if they are on the same layer 2 network. This method involves minimal network overlay, with containers using IP addresses from the node pool.\nAdvantages:\nSimplicity: Easier to understand and troubleshoot compared to more complex network setups like overlays. Performance: Potentially lower latency and overhead since there’s no need for encapsulation or additional routing. Disadvantages:\nScalability: Limited by the size of the subnet, making it less suitable for large clusters. IP Management: Requires diligent management of IP addresses to prevent conflicts and ensure sufficient addresses for all pods. Some common CNI plugins that can support flat network configurations include Cilium in native mode.\nOverlay Network CNI:\nAbstracts the physical network by creating a virtual network layer, encapsulating container traffic within tunnels (e.g., VXLAN). This provides strong isolation between network segments.\nAdvantages:\nScalability: Can support large clusters by isolating pod IPs from the underlying physical network. Network Isolation: Provides better isolation and security between different network segments or tenants. Flexibility: Allows for more complex networking setups, including multi-tenancy and cross-datacenter connectivity. Disadvantages:\nPerformance Overhead: Encapsulation can introduce additional latency and CPU overhead due to the processing required. Complexity: More complex to configure, troubleshoot, and manage compared to flat networks. Some popular CNI plugins that support overlay networks include Cilium in encapsulation mode.\nPod-to-Pod Communication # When pods are on the same node, they communicate through the host\u0026rsquo;s network bridge at Layer 2 (L2) of the OSI model. See previews article.\nNetwork Bridge:\nEach node in Kubernetes has a network bridge that acts as a virtual switch, connecting all the pods on that node. Pods on the same node have virtual Ethernet interfaces connected to this bridge. When a pod sends a packet to another pod on the same node, the packet is sent to the network bridge, which then forwards it to the destination pod without leaving the node. The communication happens directly at the Ethernet frame level, making it very fast and efficient. No IP routing is involved, which reduces overhead and latency.\nExample:\nPod 1 (IP: 172.16.0.30) sends a packet to Pod 2 (IP: 172.16.0.40) on the same node. The packet is sent to the network bridge, which forwards it directly to Pod 2 based on its MAC address. When pods are on different nodes, they communicate via Layer 3 routing, involving default gateways and potentially an overlay network.\nDefault Gateway:\nEach node has a default gateway, typically managed by the node\u0026rsquo;s network interface (often eth0).\nWhen a pod needs to communicate with another pod on a different node, it sends the packet to its default gateway.\nThe packet is encapsulated (if using an overlay network like VXLAN, the original packet is wrapped in another packet with a new header) and routed across the physical network infrastructure to the destination node.\nThe destination node\u0026rsquo;s gateway decapsulates the packet (removing the outer header and exposing the original packet) and forwards it to the appropriate pod.\nExample:\nPod 1 on Node A (IP: 172.16.0.30) wants to communicate with Pod 2 on Node B (IP: 172.17.0.30). Pod 1 sends the packet to its default gateway, which encapsulates the packet and routes it through the physical network to Node B. Node B\u0026rsquo;s gateway decapsulates the packet and forwards it to Pod 2. Overlay Network (if used):\nAn overlay network abstracts the physical network, allowing pods to communicate across different nodes as if they are on the same network.\nTechnologies like VXLAN encapsulate the pod\u0026rsquo;s traffic, providing a virtual Layer 2 network on top of the Layer 3 physical network.\nPod-to-Services # In Kubernetes, services provide a stable IP address and DNS name for a set of pods, allowing clients to reliably communicate with the pods regardless of their IP addresses. Here’s how pod-to-service communication works:\nKubernetes Services\nWhen a service is created, Kubernetes assigns it a stable IP address (ClusterIP) and, optionally, an external IP address.\nServices use selectors to match the pods that should receive traffic.\nKubernetes automatically creates endpoint objects for a service, which track the IP addresses and ports of the pods that match the service’s selectors.\nNetworking Components\nKubernetes services rely on two essential networking components:\nnetfilter is a framework within the Linux kernel that performs various network-related operations, including packet filtering, NAT (Network Address Translation), and port forwarding. iptables is a user-space utility program that allows administrators to configure the netfilter tables with rules for how packets should be handled. Pod-to-Service Communication\nThe pod1 initiates a request to the service\u0026rsquo;s ClusterIP.\nUpon reaching the node\u0026rsquo;s network stack (node B), iptables rules come into play. Kubernetes configures several iptables chains to manage service traffic, primarily within the nat table under chains like KUBE-SERVICES and KUBE-POSTROUTING.\nroot@kind-worker:/# iptables -t nat --list ... Chain POSTROUTING (policy ACCEPT) target prot opt source destination KUBE-POSTROUTING all -- anywhere anywhere /* kubernetes postrouting rules */ DOCKER_POSTROUTING all -- anywhere ubuntu KIND-MASQ-AGENT all -- anywhere anywhere ADDRTYPE match dst-type !LOCAL /* kind-masq-agent: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom KIND-MASQ-AGENT chain */ ... Chain KUBE-SERVICES (2 references) target prot opt source destination KUBE-SVC-JD5MR3NA4I4DYORP tcp -- anywhere 10.96.0.10 /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153 KUBE-SVC-S6WXVSJHNSEXRU24 tcp -- anywhere 10.96.254.54 /* default/sampleservice cluster IP */ tcp dpt:http KUBE-SVC-NPX46M4PTMTKRN6Y tcp -- anywhere 10.96.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:https KUBE-SVC-TCOU7JCQXEZGVUNU udp -- anywhere 10.96.0.10 /* kube-system/kube-dns:dns cluster IP */ udp dpt:domain KUBE-SVC-ERIFXISQEP7F7OF4 tcp -- anywhere 10.96.0.10 /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain KUBE-NODEPORTS all -- anywhere anywhere /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL Utilizing DNAT, iptables translates the service\u0026rsquo;s ClusterIP and port to one of the service\u0026rsquo;s endpoints (pod IP and port) in a round-robin fashion.\nThis translation is executed via KUBE-SVC-\u0026lt;ServiceHash\u0026gt; chains, with each service having a unique hash and a corresponding chain in iptables.\nroot@kind-worker:/# iptables -t nat --list ... Chain KUBE-SVC-S6WXVSJHNSEXRU24 (2 references) target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.96.254.54 /* default/sampleservice cluster IP */ tcp dpt:http KUBE-SEP-QOULV7SDHVBTRDDQ all -- anywhere anywhere /* default/sampleservice -\u0026gt; 10.244.1.2:80 */ ... The netfilter framework processes the packet based on the iptables rules, executing the necessary NAT operations to modify the packet’s destination IP and port. The packet is then directed towards the appropriate pod IP.\nThe modified packet arrives at the destination pod, which processes the request and generates a response.\nThe response packet travels back to the client pod, potentially undergoing SNAT (Source NAT) by iptables to ensure the packet appears to originate from the service IP, thus maintaining transparency for the client pod.\nConclusion # This detailed exploration of the steps and interactions involved in pod creation and network traffic in Kubernetes highlights the intricate orchestration that ensures efficient, secure, and isolated container operations. Understanding these processes provides insight into Kubernetes\u0026rsquo; powerful container management capabilities.\n","date":"May 24, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/networking/kubernetes-networking/","section":"Articles","summary":"This article delves into the underlying processes involved in creating a pod and traces the path of network traffic within a Kubernetes cluster, from an initial web request to the container hosting the application. We will explore:\nWhat steps involved in the pod creation How containers communicate locally (Intra-Pod communication). Pod-to-Pod communication when the pods are on the same and different nodes. Pod-to-Service communication, where a pod sends traffic to another pod behind a service in Kubernetes.","title":"Decoding Kubernetes Pod Creation and Network Traffic Management","type":"articles"},{"content":"","date":"May 24, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"May 24, 2024","externalUrl":null,"permalink":"/website/.pr/103/series/networking/","section":"Series","summary":"","title":"Networking","type":"series"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/bridge/","section":"Tags","summary":"","title":"Bridge","type":"tags"},{"content":" Introduction # Container networking might seem complex and almost mystical initially, but it\u0026rsquo;s actually built on basic Linux networking principles. By understanding these fundamentals, we can troubleshoot the container networking layers on a profound level. Furthermore, we might even create container networking solution from scratch for pure enjoyment.\nIn this article we are going to cover the foundational elements of container networking, from the underlying principles of network namespaces to the practical tools and techniques for managing container networking environments.\nRequirements # For this article, you\u0026rsquo;ll require a Linux host system equipped with the following tools: nsenter, ping, and tcpdump.\nDive Into The Foundations of Container Networking # A container is essentially, an isolated and restricted Linux process within its own networking context that is completely separate from both the host system and other containers.\nAt the heart of this isolation lies a fascinating Linux kernel feature known as network namespaces. According to the man page, \u0026ldquo;network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.\u0026rdquo;.\nEssentially, it allows processes to operate within their own distinct network environment, including having their own network interfaces, routing tables, and firewall configurations.\nThis capability of Linux kernel empowers containerization technologies like Docker. By placing containers within their own network namespaces, they gain autonomy over their network configurations and ensures that containers can communicate with each other or the external network without causing interference with other containers or the host system\u0026rsquo;s network setup (root namespace).\nTo unravel this concept, let\u0026rsquo;s delve into some fundamental aspects.\nWhat constitutes a Networking Stack? # The networking stack is a set of software layers designed to facilitate communication across networks. Each layer within this stack is responsible for specific tasks such as data transmission, addressing, routing, and interaction with applications. Together, these layers collaborate to establish communication between devices connected to a network. This networking stack typically follows the OSI (Open Systems Interconnection) model.\nHere\u0026rsquo;s a overview of the layers commonly present in a networking stack:\nEach layer of the networking stack interacts with the layers above and below it, encapsulating data as it moves down the stack and decapsulating it as it moves up. In this article, we\u0026rsquo;ll relay on the key components of the network stack, including:\nNetwork Devices: Network devices, such as network interface cards (NICs), switches, and routers, are physical or virtual entities responsible for transmitting and receiving data packets on a network. These devices interface with the Linux networking stack to exchange data with other devices. You can list these devices using the command ip link list.\nRouting Table: The routing table is a critical component of the networking stack that contains information about the available routes to different network destinations. When a packet arrives at a Linux system, the networking stack consults the routing table to determine the appropriate path for forwarding the packet to its destination. Viewable with ip route list.\niptables Rules: iptables is a powerful firewall utility in Linux that allows administrators to define rules for packet filtering and network address translation (NAT). These rules are organized into chains within the iptables framework.\nThe sequence of rules are applied to packets as they traverse the networking stack of a Linux system. These rules dictate how packets are processed, filtered, and potentially modified as they move through various stages of the networking stack.\nThere are three main built-in chains in iptables, each corresponding to a different stage of packet processing:\nINPUT: Used for packets destined for the local system. FORWARD: Used for packets passing through the system. OUTPUT: Used for packets originating from the local system. When a packet reaches a chain, it is compared against the rules defined in that chain. Each rule specifies criteria that the packet must match (such as source or destination IP address, port number, protocol, etc.), as well as an action to take if the packet matches the criteria (such as accept, drop, reject, or forward to another chain).\nIn addition to these three built-in chains, users can also create custom chains to organize rules more efficiently or to perform specific tasks. iptables --list-rules.\nWait of our next article for more details on iptables.\nInspect the Network Environment # Now let\u0026rsquo;s apply what we learned before and inspect the network environment before running containers.\nNetwork devices:\nroot@ubuntu:~# ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff Routing table:\nroot@ubuntu:~# ip route list default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 iptable rules:\nroot@ubuntu:~# iptables --list-rules -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER ... Note: If Docker is running on your host system, you will notice some custom chains added to your iptables rules.\nCreate A Network Namespace # Linux offers various tools and utilities for network namespace management. The ip netns command facilitates the creation, deletion, and the configuration of network namespaces.\nCreate a new network namespace:\nroot@ubuntu:~# ip netns add app1 To display the network namespaces that have been created, you can use the following command:\nroot@ubuntu:~# ip netns list app1 To execute a process within a network namespace, you can utilize the nsenter utility. Here\u0026rsquo;s how you can initiate a new shell within the app1 namespace:\nroot@ubuntu:~# nsenter --net=/run/netns/app1 bash Now that we\u0026rsquo;ve created a Bash process in the app1 namespace, we can execute our commands within the network namespace.\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 root@ubuntu:~# ip route list root@ubuntu:~# iptables --list-rules -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT This output indicates that the Bash process is operating within a fully isolated namespace (app1). There are no routing rules or custom iptables chains present, and only the loopback interface is available.\nNote: loopback interface is a virtual network interface that allows communication between applications running on the same network namespace.\nNetwork Namespace to Host connectivity # Remember, network namespaces are isolated from each other and from the host. Now, let\u0026rsquo;s establish a connection between the container\u0026rsquo;s network namespace app1 and the host.\nTo accomplish this, we\u0026rsquo;ll use virtual Ethernet devices (veth). A veth device is a type of virtual network interface in Linux, creating a virtual network link between two network namespaces or between a network namespace and the host system.\nHere\u0026rsquo;s how it works:\nA veth device is typically configured as a pair of virtual network interfaces. One end of the pair resides within the container\u0026rsquo;s network namespace, while the other end is situated either in another network namespace or directly within the host system.\nOpen a new shell session to access the root network namespace:\nroot@ubuntu:~# ip link add hlink1 type veth peer name clink1 This command creates a pair of virtual Ethernet interfaces veth, named hlink1 and clink1, within the root network namespace. You can observe the outcome by listing the network devices using the following command.\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff ... 9: clink1@hlink1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff 10: hlink1@clink1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff The output shows that both hlink1 and clink1 interfaces are located within the root network namespace and the state of each one is DOWN.\nTo establish a connection between the root network namespace and the app1 network namespace, we must retain one of the interfaces, hlink1, in the root namespace while relocating the other one, clink1, into the app1 namespace (use the following command).\nroot@ubuntu:~# ip link set clink1 netns app1 Again, list the network interfaces in the root network namespace:\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff ... 10: hlink1@if9: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff link-netns app1 The output shows that the clink1 interface has been removed from the network device list within the root network namespace.\nNow, check the network device list in app1 namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip link list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: clink1@if10: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 The clink1 interface has been moved from the root network namespace to app1. Notice the inclusion of link-netns app1 in the hlink1 interface within the root network namespace\u0026rsquo;s device list output. This indicates that the hlink1 interface is now linked with the app1 network namespace.\nNote: keep in mind that both interfaces, clink1 and lo, in app1 network namespace are DOWN.\nCurrently, each end of the veth pair resides in its respective namespace to establish the connection between both network namespaces. However, the link cannot yet be utilized for communication between the namespaces until we assign suitable IP addresses to the different interfaces and activate them.\nLet\u0026rsquo;s assigne 172.16.0.11/16 to clink1 within app1 network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 bash -c \u0026#39; ip link set clink1 up ip addr add 172.16.0.11/16 dev clink1 \u0026#39; Review the configuration of clink1 within the app1 network namespace to inspect the assignment of IP addresses.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip addr show dev clink1 9: clink1@if10: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.16.0.11/16 scope global clink1 valid_lft forever preferred_lft forever Additionally, we must setup the loopback interface within the app1 network namespace. This ensures seamless communication among processes within the network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip link set lo up Now, returning to the root network namespace, let\u0026rsquo;s assign the IP address 172.16.0.10/16 to the hlink1 interface.\nroot@ubuntu:~# ip link set hlink1 up root@ubuntu:~# ip addr add 172.16.0.10/16 dev hlink1 Inspect the hlink1 configuration within the root network namespace to see the IP assignment.\nroot@ubuntu:~# ip addr show dev hlink1 10: hlink1@if9: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff link-netns app1 inet 172.16.0.10/16 scope global hlink1 valid_lft forever preferred_lft forever We\u0026rsquo;ve successfully established the link between the app1 network namespace and the root, as illustrated in the diagram above. Now, let\u0026rsquo;s inspect the routing table within the app1 and root network namespace to understand how traffic originating from app1 namespace will be routed to reach the root.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip route 172.16.0.0/16 dev clink1 proto kernel scope link src 172.16.0.11 The output shows that traffic distinated for 172.16.0.0/16 will be routed through the clink1 interface. Given that clink1 is one end of the veth pair, it implies that any traffic traversing this interface will also be observable on the other end of the link hlink1 within the root namespace.\nThen the routing table of the root network namespace.\nroot@ubuntu:~# ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 The route 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 indicates that traffic destined for 172.16.0.0/16 will be routed through the hlink1 interface. Since hlink1 is one end of the veth pair, it also means that any traffic passing through this interface will also be visible on the other end of the link, clink1, within the app1 network namespace.\nLet\u0026rsquo;s start our first connectivity test by pinging hlink1 from the app1 network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ping 172.16.0.10 PING 172.16.0.10 (172.16.0.10) 56(84) bytes of data. 64 bytes from 172.16.0.10: icmp_seq=1 ttl=64 time=0.224 ms 64 bytes from 172.16.0.10: icmp_seq=2 ttl=64 time=0.045 ms And now, clink1 from the root network namespace.\nroot@ubuntu:~# ping 172.16.0.11 PING 172.16.0.11 (172.16.0.11) 56(84) bytes of data. 64 bytes from 172.16.0.11: icmp_seq=1 ttl=64 time=0.047 ms 64 bytes from 172.16.0.11: icmp_seq=2 ttl=64 time=0.060 ms The connectivity test has been successful between root network namespace and app1.\nNetwork Namespace to Network Namespace connectivity # In this section, we\u0026rsquo;ll demonstrate that relying only on veth pairs isn\u0026rsquo;t enough to interconnect two or more containers. This leads to conflicts in the Layer 3 (L3) routing table caused by introducing separate rules for different network namespaces in root namespace routing table. Let\u0026rsquo;s create a second network namespace app2 and configure the veth pair as in the app1:\nip netns add app2 ip link add hlink2 type veth peer name clink2 ip link set hlink2 up ip addr add 172.16.0.20/16 dev hlink2 ip link set clink2 netns app2 nsenter --net=/run/netns/app2 bash -c \u0026#39; ip link set lo up ip link set clink2 up ip addr add 172.16.0.21/16 dev clink2 exit \u0026#39; Now, check the routing table of the root network namespace:\nroot@ubuntu:~# ip route ... 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 # app1 172.16.0.0/16 dev hlink2 proto kernel scope link src 172.16.0.20 # app2 ... As you can see, the root network namespace\u0026rsquo;s routing table encountered a conflict. After adding the second veth pair, a new route was added: 172.16.0.0/16 dev hlink2 proto kernel scope link src 172.16.0.20. However, there was already an existing route for the 172.16.0.0/16 network. Consequently, when app2 tries to ping the hlink2 device, the first route is chosen, leading to connectivity issues.\nConnectivity test again.\nroot@ubuntu:~# nsenter --net=/run/netns/app2 ping 172.16.0.20 # ping root\u0026#39;s interface hlink2 PING 172.16.0.20 (172.16.0.20) 56(84) bytes of data. --- 172.16.0.20 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 1023ms As expected, the connectivity test confirmes that accessing hlink2 from app2 is not feasible, even with the routes present in routing tables.\nAlternatively, to confirm this, we can use tcpdump on the hlink2 interface within the root network namespace. By running the command tcpdump -i hlink2 icmp, all ICMP packets passing through the hlink2 interface are captured. However, in this scenario, no traffic will be captured.\nLinux Bridge # The previous issue pushed us to explore alternatives for interconnecting containers, leading us to the Linux feature known as Linux Bridge. Operating similarly to a network switch, a Linux bridge facilitates packet forwarding between connected interfaces at the L2 level. (Please check Setting up Load Balancer Service with Cilium in KinD Cluster for more details on l2 connectivity)\nIn the container ecosystem, the bridge acts as a virtual networking interface, linking container\u0026rsquo;s network namespace through virtual Ethernet pairs. Each network namespace is equipped with its own veth pair, with one end attached to the bridge network. This setup facilitates internal communication between containers and with the host system.\nIn this section, we will connect two new network namespaces to each other using a bridge. To begin, create two new network namespaces named app3 and app4.\nip netns add app3 ip link add hlink3 type veth peer name clink3 ip link set hlink3 up ip link set clink3 netns app3 nsenter --net=/run/netns/app3 bash -c \u0026#39; ip link set lo up ip link set clink3 up ip addr add 172.16.0.30/16 dev clink3 ip link \u0026#39; ip netns add app4 ip link add hlink4 type veth peer name clink4 ip link set hlink4 up ip link set clink4 netns app4 ip link nsenter --net=/run/netns/app4 bash -c \u0026#39; ip link set lo up ip link set clink4 up ip addr add 172.16.0.40/16 dev clink4 ip link \u0026#39; Make sure that there is no new routes added to the routing table of the root namespace:\nroot@ubuntu:~# ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 Now, let\u0026rsquo;s create the bridge device and connect hlink3 and hlink4 to it to ensure connectivity between the containers.\nip link add br0 type bridge ip link set br0 up ip link set hlink3 master br0 ip link set hlink4 master br0 ip link Connectivity Test # Note: If you are running docker in your host, make sure to remove br_netfilter kernel module rmmod br_netfilter (for more details https://unix.stackexchange.com/questions/671644/cant-establish-communication-between-two-network-namespaces-using-bridge-networ)\nBefore conducting the ping test, make sure to run tcpdump on the bridge interface to capture the ICMP traffic passing through the bridge while both containers, app3 and app4 communicate.\ntcpdump -i br0 icmp Now, switch to another shell session and run the following connectivity tests:\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ping -c 2 172.16.0.40 # ping app4 PING 172.16.0.40 (172.16.0.40) 56(84) bytes of data. 64 bytes from 172.16.0.40: icmp_seq=1 ttl=64 time=0.142 ms 64 bytes from 172.16.0.40: icmp_seq=2 ttl=64 time=0.071 ms --- 172.16.0.40 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1025ms rtt min/avg/max/mdev = 0.071/0.106/0.142/0.035 ms root@ubuntu:~# nsenter --net=/run/netns/app4 ping -c 2 172.16.0.30 # ping app3 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. 64 bytes from 172.16.0.30: icmp_seq=1 ttl=64 time=0.053 ms 64 bytes from 172.16.0.30: icmp_seq=2 ttl=64 time=0.057 ms --- 172.16.0.30 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1028ms rtt min/avg/max/mdev = 0.053/0.055/0.057/0.002 ms We can see from the tcpdump output bellow, that the ICMP traffic if going through brigde interface to reach each destination network namespace.\nroot@ubuntu:~# tcpdump -i br0 icmp tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on br0, link-type EN10MB (Ethernet), snapshot length 262144 bytes 16:19:13.037613 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo request, id 44793, seq 1, length 64 16:19:13.037626 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo reply, id 44793, seq 1, length 64 16:19:14.062746 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo request, id 44793, seq 2, length 64 16:19:14.062781 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo reply, id 44793, seq 2, length 64 16:19:42.217807 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo request, id 13480, seq 1, length 64 16:19:42.217839 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo reply, id 13480, seq 1, length 64 16:19:43.245866 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo request, id 13480, seq 2, length 64 16:19:43.245894 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo reply, id 13480, seq 2, length 64 The test is a success!\nRemember, we did not assigne any IP addresses for hlink3 and hlink4; instead, we assigned IPs only to clink3 and clink4. As hlink3 and hlink4 belong to the same Ethernet segment (connected to the bridge), L2 connectivity is established based on MAC addresses. This is further confirmed by inspecting the ARP table in both namespaces.\nARP (Address Resolution Protocol) table in Linux offers a mapping between IP addresses and MAC addresses of devices on the local network segment (L2 segment).\nWhen two devices within the same L2 segment, such as app3 and app4 connected via the bridge, need to communicate, they rely on MAC addresses. The ARP table entries play a crucial role in this process by actively providing the necessary MAC destination address for constructing the L2 frame.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 arp -n Address HWtype HWaddress Flags Mask Iface 172.16.0.40 ether 52:45:d1:3a:b3:32 C clink3 In the ARP table of app3, we observe that the destination MAC address for app4 is 52:45:d1:3a:b3:32, corresponding to the destination IP address of 172.16.0.40.\nSame in app4.\nroot@ubuntu:~# nsenter --net=/run/netns/app4 arp -n Address HWtype HWaddress Flags Mask Iface 172.16.0.30 ether 82:6e:c9:b4:c0:79 C clink4 Note: To get the MAC address of each network interface, execute ip addr show INTF_NAME.\nAlternatively, the ip neigh command offers information about the neighbor cache.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ip neigh 172.16.0.40 dev clink3 lladdr 52:45:d1:3a:b3:32 REACHABLE In this specific output, we see that app3 has a neighbor device with the IP address 172.16.0.40 (app4), accessible through the clink3 interface. The MAC address (L2 address) of this neighbor device is identified as 52:45:d1:3a:b3:32. Furthermore, the state of this entry in the neighbor cache is marked as REACHABLE, indicating that the MAC address is presently known and accessible.\nAfter establishing the connection between app3 and app4, it\u0026rsquo;s now time to enssure the connectivity between the host (root network namespace) and both app3 and app4. First, let\u0026rsquo;s attempt to ping app3 from the root network namespace.\nroot@ubuntu:~# ping -c 2 172.16.0.30 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. From 172.16.0.10 icmp_seq=1 Destination Host Unreachable From 172.16.0.10 icmp_seq=2 Destination Host Unreachable --- 172.16.0.30 ping statistics --- 2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 1017ms The root namespace is currently unable to communicate with the app3 network namespace because there\u0026rsquo;s no route in root\u0026rsquo;s routing table allowing to reach app3. To fix this, we first need to assign an IP address to the bridge network interface itself.\nip addr add 172.16.0.1/16 dev br0 Assigning an IP address to br0 updates the root namespace routing table with the route: 172.16.0.0/16 dev br0 proto kernel scope link src 172.16.0.1 (you can see it by running ip route in root network namespace). This indicates that traffic destined for the 172.16.0.0/16 subnet can be routed via the br0 interface. Given that hlink1 and hlink2 are also connected to the bridge, this implies that traffic will be appropriately directed to the target namespaces.\nRemember, the bridge operates at the data link layer (L2), so once traffic is routed to the bridge interface, an ARP request is broadcasted to all devices connected to the bridge, requesting the destination MAC address. The device that possesses the MAC address will respond, and the Ethernet frame will be filled with the destination MAC address before being sent to its destination within this L2 segment.\nAfter this configuration, we should be able to ping app3 and app4 from the root network namespace.\nroot@ubuntu:~# ping -c 2 172.16.0.30 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. 64 bytes from 172.16.0.30: icmp_seq=1 ttl=64 time=0.085 ms 64 bytes from 172.16.0.30: icmp_seq=2 ttl=64 time=0.069 ms --- 172.16.0.30 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1016ms rtt min/avg/max/mdev = 0.069/0.077/0.085/0.008 ms Aditionnaly, we also need to add a default route to app3 and app4. This means that when a destination IP address of a packet does not match any specific routes in the routing table, it will go througt the default route, the bridge interface in this case.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ip route add default via 172.16.0.1 root@ubuntu:~# nsenter --net=/run/netns/app4 ip route add default via 172.16.0.1 Now, app3 and app4 can reach any subnet within the host. Try to ping eth0 for example.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ping -c 2 192.168.64.3 PING 192.168.64.3 (192.168.64.3) 56(84) bytes of data. 64 bytes from 192.168.64.3: icmp_seq=1 ttl=64 time=0.184 ms 64 bytes from 192.168.64.3: icmp_seq=2 ttl=64 time=0.066 ms --- 192.168.64.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1019ms rtt min/avg/max/mdev = 0.066/0.125/0.184/0.059 ms Great! We are able to go from the containers to the host and back.\nInternet Connectivity # The standard method for connecting a network namespace to the internet involves Network Address Translation (NAT). NAT is a networking technique that modifies the source or destination IP addresses of packets as they travel through a router. It allows multiple devices within a local network to share a single public IP address when accessing online resources.\nTo achieve this, we must first enable packet forwarding, which is typically disabled by default in Linux. Enabling it, usually from the root namespace, transforms the host machine into a router, with the bridge interface acting as the default gateway for the containers.\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Packets originating from the network namespace will have their source IP addresses replaced with the host\u0026rsquo;s external interface address. Additionally, the host will track all existing mappings and restore the IP addresses before forwarding packets back to the containers. The following command will enable this.\niptables -t nat -A POSTROUTING -s 172.16.0.0/16 ! -o br0 -j MASQUERADE This iptables rule performs NAT for outgoing packets from the 172.16.0.0/16 subnet, excluding those destined for the \u0026ldquo;br0\u0026rdquo; interface. It dynamically translates the source IP addresses of these packets to the IP address of the host\u0026rsquo;s external interface, allowing internal IPs to appear as originating from the host\u0026rsquo;s public IP address.\nDocker Networks # In the context of Docker, a Linux bridge is a common networking solution used to connect containers. Docker creates a bridge network by default for each Docker daemon (docker0).\nEach container is attached to this bridge network, which allows them to communicate with each other. Docker creates virtual Ethernet pairs to connect containers to the bridge network. Each container gets its own network namespace, ensuring network isolation.\nDocker has also other network types:\nHost network: Docker also offers the option to use the host\u0026rsquo;s networking namespace instead of creating a separate network namespace for each container. In this mode, containers share the network namespace with the host, bypassing network isolation but potentially offering better performance.\nNone: In this mode, Docker completely isolates the network namespace from the host. It configures only the loopback interface within the network namespace, ensuring that the container operates in a fully isolated networking environment.\nOverlay Networks: Docker supports overlay networks, which enable communication between containers running on different hosts. Overlay networks use VXLAN encapsulation (wait for our next article for more details on this topic) to extend layer 2 networking across hosts.\nCustom Networks: Docker allows users to create custom networks with specific configurations. These custom networks provide additional flexibility and control over container networking.\nConclusion # Understanding Linux networking features is fundamental to many containerization technologies. This foundational knowledge serves as a basis for comprehending higher-level networking concepts used in Kubernetes. In our upcoming article, we\u0026rsquo;ll explore how Container Runtime Interface (CRI) and Container Network Interface (CNI) leverage these features within the Kubernetes ecosystem.\nYou\u0026rsquo;ll discover a script containing all the commands demonstrated in this article here. Also, consider using codespaces to run the lab.\n","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/networking/diving-deep-into-container-networking/","section":"Articles","summary":"Introduction # Container networking might seem complex and almost mystical initially, but it\u0026rsquo;s actually built on basic Linux networking principles. By understanding these fundamentals, we can troubleshoot the container networking layers on a profound level. Furthermore, we might even create container networking solution from scratch for pure enjoyment.\nIn this article we are going to cover the foundational elements of container networking, from the underlying principles of network namespaces to the practical tools and techniques for managing container networking environments.","title":"Diving deep into Container Networking (An Exploration of Linux Network Namespace)","type":"articles"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/103/series/offline-kubernetes-validation/","section":"Series","summary":"","title":"Offline Kubernetes Validation","type":"series"},{"content":"In this article, we will explore kubeconform, the successor to kubeval.\nKubeconform is based on the same principles as kubeval, but it is actively maintained and has some unique features.\nIntroduction # As the two tools are very similar, I invite you to consult the article on kubeval which also applies to kubeconform.\nResource Schemas # Kubeconform uses JSON schemas to validate (or invalidate) the proposed resources. These JSON schemas are created from the OpenAPI schemas published within the Kubernetes GitHub repository.\nUnlike kubeval, which is no longer maintained, kubeconform maintains an up-to-date repository of JSON schemas converted from the native schemas published by Kubernetes (kubeconform fork of kubernetes-json-schema). The most recent version supported as of today is v1.30.0.\nInstalling kubeconform # The kubeconform project documentation suggests installing the binary directly or using a package manager, or using it through a Docker image.\nFor this article, we will use brew to install it locally:\nbrew install kubeconform The docker image can be found at https://ghcr.io/yannh/kubeconform.\nValidation with kubeconform # Unlike kubeval, kubeconform supports the validation of custom resources, provided that you supply the corresponding JSON schemas. We will see more about this later.\nLet’s start by validating native resources.\nValidating native resources # Let\u0026rsquo;s try to validate the simple (yet invalid) manifest below:\napiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 template: spec: containers: - image: nginx name: nginx If we run kubeconform without any option the DaemonSet will be considered invalid:\nkubeconform manifests.yaml manifests.yaml - DaemonSet nginx-ds is invalid: problem validating schema. Check JSON formatting: jsonschema: \u0026#39;/spec\u0026#39; does not validate with https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master-standalone/daemonset-apps-v1.json#/properties/spec/required: missing properties: \u0026#39;selector\u0026#39; As noted above, the selector field is missing. Let\u0026rsquo;s provide a valid manifest:\napiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 selector: matchLabels: name: nginx-ds template: metadata: labels: name: nginx-ds spec: containers: - image: nginx name: nginx This time, running kubeconform manifests.yaml passes. Still, the unexpected replicas field is not detected.\nTo make kubeconform detect unexpected fields, we need to specify --strict:\nkubeconform --strict manifests.yaml manifests.yaml - DaemonSet nginx-ds is invalid: problem validating schema. Check JSON formatting: jsonschema: \u0026#39;/spec\u0026#39; does not validate with https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master-standalone-strict/daemonset-apps-v1.json#/properties/spec/additionalProperties: additionalProperties \u0026#39;replicas\u0026#39; not allowed Corner cases # As explained in the article on kubeval, schema-based validation is often approximate.\nProperty type checks work quite well, but the validation of associated values is less reliable. This is because the validation logic is implemented in specific code and does not always form part of the schema definition.\nLet\u0026rsquo;s take the example of an invalid label:\napiVersion: v1 kind: ConfigMap metadata: name: cm labels: # ERROR: a label key must start with an alphanumeric character _: invalid data: foo: bar Let\u0026rsquo;s try to validate the resource above with:\nkubeconform --strict manifests.yaml This time kubeconform does not detect the invalid label.\nAnother simple example is the following, which defines a Deployment with a negative number of replicas:\napiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: replicas: -1 selector: matchLabels: name: deploy template: metadata: labels: name: deploy spec: containers: - image: nginx name: nginx Here too, kubeconform --strict manifests.yaml does not detect the problem even though the resource will be rejected by a cluster:\nkubectl apply -f manifests.yaml The Deployment \u0026#34;deploy\u0026#34; is invalid: spec.replicas: Invalid value: -1: must be greater than or equal to 0 Target Kubernetes Version # In the case of native resources, the target version of the cluster is important, as native resources evolve regularly with the appearance of new properties, new API versions, and some API versions are deprecated and then removed, etc.\nTherefore, a resource considered valid given one version of Kubernetes may be considered invalid with an older version of Kubernetes.\nIt is possible to target a specific version of Kubernetes using the --kubernetes-version argument.\nLet\u0026rsquo;s take, for example, the API extensions/v1beta1 which was removed from Kubernetes in v1.22:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: frontend spec: rules: - host: frontend.minikube.local http: paths: - path: / backend: serviceName: frontend servicePort: 80 The Ingress resource above is considered valid with Kubernetes v1.20:\nkubeconform --kubernetes-version 1.20.0 --strict manifests.yaml While the schema is no longer available in v1.22:\nkubeconform --kubernetes-version 1.22.0 --strict manifests.yaml manifests.yaml - Ingress frontend failed validation: could not find schema for Ingress CRDs validation # Let\u0026rsquo;s finish with the validation of custom resources (CRDs).\nKubeconform supports custom resources provided that you can supply it with the corresponding JSON schema.\nFor instance, using an example from kubeconform\u0026rsquo;s GitHub, we can utilize Datree\u0026rsquo;s CRDs-catalog which lists the most used CRDs and provides the associated JSON schemas.\nLet\u0026rsquo;s take for example an Issuer resource of cert manager:\napiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} Without specifying the location of the schema associated with the Issuer resource type, kubeconform is unable to validate the resource:\nkubeconform --strict manifests.yaml manifests.yaml - Issuer test-selfsigned failed validation: could not find schema for Issuer In order to enable kubeconform to download the corresponding JSON schema, one can use the --schema-location argument:\nkubeconform --schema-location \u0026#39;https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json\u0026#39; --strict manifests.yaml This time, kubeconform can validate the Issuer resource using the schema downloaded from Datree\u0026rsquo;s GitHub repository.\nNote that the --schema-location argument can be specified as many times as necessary. It is also possible to convert a CRD definition yourself, which is all explained here.\nConclusion # In conclusion, kubeconform is heavily inspired by kubeval and has taken up the mantle to become its worthy successor.\nBoth tools operate in exactly the same way based on JSON schema, and kubeconform maintains an up-to-date version of the schemas for native resources.\nFinally, kubeconform adds support for custom resources, which significantly broadens the tool\u0026rsquo;s scope. Their use, however, remains quite complex, especially in the case of a custom resource whose schema is not publicly available on the internet.\nIn summary, kubeconform is a significant step forward for users of kubeval, providing a satisfactory tool for use without custom resources. However, it remains limited in the case of intensive use of CRDs.\nStay tuned! # The next article will explore a tool more suited to modern Kubernetes usage with enhanced support for custom resources. Stay tuned for the next episode!\n","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/k8s-resources-validation/kubeconform-review/","section":"Articles","summary":"In this article, we will explore kubeconform, the successor to kubeval.\nKubeconform is based on the same principles as kubeval, but it is actively maintained and has some unique features.\nIntroduction # As the two tools are very similar, I invite you to consult the article on kubeval which also applies to kubeconform.\nResource Schemas # Kubeconform uses JSON schemas to validate (or invalidate) the proposed resources. These JSON schemas are created from the OpenAPI schemas published within the Kubernetes GitHub repository.","title":"Validating Kubernetes resources offline - kubeconform review","type":"articles"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/validation/","section":"Tags","summary":"","title":"Validation","type":"tags"},{"content":"In this article, we will discuss validating Kubernetes resource definitions using the tool kubeval.\nBefore we begin, it\u0026rsquo;s important to note that kubeval is no longer maintained, and the author advises migrating to kubeconform. We will explore kubeconform in the next article.\nIntroduction # Validating Kubernetes resources is very useful for detecting non-compliant resources before applying them to a cluster. The most common use case is to perform validation during CI to validate (or invalidate) the quality of a pull request.\nIn this specific case, a cluster is generally not available, and validation must therefore be performed offline. The tool must be able to function autonomously and validate (or invalidate) the supplied manifests.\nKubernetes-side Validation # Validation within a Kubernetes cluster is performed differently depending on the types of resources used.\nFor native Kubernetes resources (ConfigMap, Deployment, Pod, etc.), validation is done by dedicated code. The validation logic is therefore hard to leverage as it is not expressed in a declarative manner.\nNext comes the case of custom resources (described using CRDs). These types of resources are not native and must adhere to a more or less precise schema. This schema is used by Kubernetes to validate a resource submitted to it.\nFinally, it is also possible to enrich the validation logic by adding webhooks invoked by the Kubernetes API server at the time of resource admission.\nResource Schemas # The schemas for native or custom resources are expressed using OpenAPI v2 (or v3 for more recent versions of Kubernetes). These schemas are more or less precise in the case of native resources, with the validation logic being implemented in dedicated code.\nThese schemas are used by most resource validation tools to validate (or not) the proposed resources.\nValidation Webhooks # Until recently, validation webhooks were standard services, internal or external to the cluster, invoked by the API server during the admission of a resource.\nThese webhooks could implement a completely customized logic, and it is generally excluded that a validation tool could reproduce the logic embedded in these different services.\nRecently, the introduction of CEL and validation policies have allowed programming the behavior of the API server in a declarative manner. Validation tools are theoretically able to apply the same validation policies as the API server.\nInstalling kubeval # The kubeval project documentation suggests installing the binary directly or using a package manager, or using it through a Docker image.\nFor this article, we will use the Docker image, as installation with brew failed since the project is no longer maintained \u0026#x1f937;\nValidation with kubeval # First, let\u0026rsquo;s note that kubeval does not support validation of CRDs. The recommended workaround is to exclude custom (unknown) resources.\nLet\u0026rsquo;s try to validate the simple (yet invalid) manifest below:\napiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 template: spec: containers: - image: nginx name: nginx If we run kubeval without any option the DaemonSet will be considered valid:\ndocker run -it -v ${PWD}:/work garethr/kubeval work/manifests.yaml PASS - work/manifests.yaml contains a valid DaemonSet (nginx-ds) To make kubeval detect additional fields, we need to specify --strict:\ndocker run -it -v ${PWD}:/work garethr/kubeval work/manifests.yaml --strict WARN - work/manifests.yaml contains an invalid DaemonSet (nginx-ds) - replicas: Additional property replicas is not allowed Target Kubernetes Version # In the case of native resources, the target version of the cluster is important, as native resources evolve regularly with the appearance of new properties, new API versions, some API versions are deprecated and then removed, etc.\nTherefore, a resource considered valid given one version of Kubernetes may be considered invalid with an older version of Kubernetes.\nIt is possible to target a specific version of Kubernetes using the -v argument. Unfortunately, the tool relies on schemas published on the website https://kubernetesjsonschema.dev which stopped at version 1.18 of Kubernetes.\nConclusion # It seems somewhat irrelevant to push the study of kubeval much further. It is clear that the tool is outdated, is no longer maintained, is at least 10 versions of Kubernetes behind, and has therefore clearly become obsolete.\nIt remains, however, the first tool to have popularized offline validation of Kubernetes resources and it deserved to be mentioned.\nWe will see other more modern tools in future articles. Stay tuned for the next episode!\n","date":"May 5, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/k8s-resources-validation/kubeval-review/","section":"Articles","summary":"In this article, we will discuss validating Kubernetes resource definitions using the tool kubeval.\nBefore we begin, it\u0026rsquo;s important to note that kubeval is no longer maintained, and the author advises migrating to kubeconform. We will explore kubeconform in the next article.\nIntroduction # Validating Kubernetes resources is very useful for detecting non-compliant resources before applying them to a cluster. The most common use case is to perform validation during CI to validate (or invalidate) the quality of a pull request.","title":"Validating Kubernetes resources offline - kubeval review","type":"articles"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/cilium/","section":"Tags","summary":"","title":"Cilium","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/kind/","section":"Tags","summary":"","title":"KinD","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/load-balancer/","section":"Tags","summary":"","title":"Load Balancer","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/103/tags/macos/","section":"Tags","summary":"","title":"MacOS","type":"tags"},{"content":"Kubernetes in Docker (KinD) offers a lightweight and efficient way to run Kubernetes clusters for development and testing purposes. However, setting up KinD with load balancing option requires specific networking configurations. In this article, we\u0026rsquo;ll explore the networking configuration of KinD on both Linux and MacOS, deep dive into load balancing options and discuss troubleshooting tactics.\nRequirements # Docker KinD Kubectl Ciliumctl Setting Up Kubernetes in Docker # To create a KinD cluster with two nodes, you can use the following configuration:\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: disableDefaultCNI: true kubeProxyMode: none nodes: - role: control-plane - role: worker Verify the node status\nkubectl get nodes NAME STATUS ROLES AGE VERSION kind-control-plane NotReady control-plane 31s v1.29.2 kind-worker NotReady \u0026lt;none\u0026gt; 10s v1.29.2 The nodes are in a \u0026lsquo;NotReady\u0026rsquo; state because the CNI installation is disabled in the KinD config file, where \u0026lsquo;disableDefaultCNI\u0026rsquo; is set to true. This means that the essential networking layer required for pod communication and cluster operation is not configured. Consequently, the kubelet reports a \u0026ldquo;NotReady\u0026rdquo; state as it cannot establish network connectivity. Without CNI, pods cannot be assigned IP addresses.\nIn this article, we are going to use Cilium CNI. Cilium offers advanced networking features, enhanced security, service mesh integration, scalability, and comprehensive observability features. In this article, we’re going to explore some of Cilium advanced networking capabilities.\nDeploying Cilium is straightforward, let\u0026rsquo;s simplify things by applying the following configuration:\ncilium install \\ --set kubeProxyReplacement=\u0026#34;strict\u0026#34; \\ --set routingMode=\u0026#34;native\u0026#34; \\ --set ipv4NativeRoutingCIDR=\u0026#34;10.244.0.0/16\u0026#34; \\ --set k8sServiceHost=\u0026#34;kind-control-plane\u0026#34; \\ --set k8sServicePort=6443 \\ --set l2announcements.enabled=true \\ --set l2announcements.leaseDuration=\u0026#34;3s\u0026#34; \\ --set l2announcements.leaseRenewDeadline=\u0026#34;1s\u0026#34; \\ --set l2announcements.leaseRetryPeriod=\u0026#34;500ms\u0026#34; \\ --set devices=\u0026#34;{eth0,net0}\u0026#34; \\ --set externalIPs.enabled=true \\ --set autoDirectNodeRoutes=true \\ --set operator.replicas=2 Once the configuration is applied, you can verify the status of the Cilium deployment by executing the command cilium status --wait. This command will display the live deployment status of various Cilium components. Afterwards, running kubectl get nodes will display the nodes in a ready state, confirming the successful setup of networking with Cilium.\nNow let\u0026rsquo;s explore network configuration inside the kubernetes cluster:\nkubectl cluster-info dump | grep -m 1 cluster-cidr \u0026#34;--cluster-cidr=10.244.0.0/16\u0026#34;, As you can see, the Kubernetes cluster is configured with a cluster CIDR range of 10.244.0.0/16. This CIDR range is used by cilium CNI for assigning IP addresses to pods within the cluster.\nThe subnets assigned to each node are:\nkubectl get nodes -o jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.spec.podCIDR}{\u0026#34;\\n\u0026#34;}\u0026#39; kind-control-plane\t10.244.0.0/24 kind-worker\t10.244.1.0/24 Every node is informed about the IP addresses of all pods on every other node, and corresponding routes are added to the Linux kernel routing table of each node. This config is clearly visible when accessing a node within the cluster. You can do this by running docker exec -it kind-worker bash then display the routing table of the node.\nroot@kind-worker:/# ip route default via 172.18.0.1 dev eth0 10.244.0.0/24 via 172.18.0.3 dev eth0 proto kernel 10.244.1.0/24 via 10.244.1.228 dev cilium_host proto kernel src 10.244.1.228 10.244.1.228 dev cilium_host proto kernel scope link 172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2 default via 172.18.0.1 dev eth0: Any traffic that doesn\u0026rsquo;t match a more specific route in the table will be sent through the gateway with the IP address 172.18.0.1 and the outgoing interface eth0.\n10.244.0.0/24 via 172.18.0.3 dev eth0 proto kernel: Route for the subnet 10.244.0.0/24 (kind-control-plan) should be routed via the gateway with the IP address 172.18.0.3 and the outgoing interface eth0.\n10.244.1.0/24 via 10.244.1.228 dev cilium_host proto kernel src 10.244.1.228: Route for the subnet 10.244.1.0/24 (kind-worker) should be sent via the gateway with the IP address 10.244.1.228 via the interface cilium_host. Additionally, it specifies that the source IP address for this traffic should be 10.244.1.228.\n10.244.1.228 dev cilium_host proto kernel scope link: Network device cilium_host with the IP address 10.244.1.228 is directly connected the local network in the container.\nCilium\u0026rsquo;s Networking options # Cilium represents a modern solution for networking and security in Kubernetes, surpassing the capabilities of traditional components like kube-proxy (disabled in cilium installation kubeProxyReplacement=\u0026quot;strict\u0026quot;). While kube-proxy focuses on basic networking tasks such as service discovery and load balancing, Cilium extends its functionality with advanced networking, security, and observability features.\nCilium offers two network configurations: encapsulation and direct routing (native routingMode=\u0026quot;native\u0026quot;), each suited to different environments and requirements. Encapsulation works well in cloud environments or situations with overlapping networks while native routing is the best option in on-premises setups or dedicated cloud environments where performance optimization is crucial.\nFor further details on this topic, refer to the Cilium documentation: Cilium Routing Concepts.\nNetwork Configuration of KinD Cluster # The architecture of the KinD cluster leverages Docker\u0026rsquo;s networking features alongside standard Kubernetes components. Within a Kind cluster, every Kubernetes node is a Docker container. These containers operate within the same linux network namespace, facilitating communication via the Docker bridge network. KinD establishes a unique bridge network for each cluster for communication between Kubernetes nodes. Let\u0026rsquo;s explore docker networks.\nmacbook-pro % docker network list NETWORK ID NAME DRIVER SCOPE 8fd3d395c77e bridge bridge local 457bca38a85e host host local 53ced5328608 kind bridge local c96ad8e9a9bb none null local When examining the network setup, you\u0026rsquo;ll likely identify two bridge networks: bridge and kind. bridge is a system-wide bridge network managed by Docker, used for all Docker containers on the host machine. In contrast, kind is specific to each Kind cluster and used exclusively by Kubernetes nodes within that cluster.\nmacbook-pro % docker inspect kind [ { \u0026#34;Name\u0026#34;: \u0026#34;kind\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;53ced532860890f57acb0f315561d6af24550e0f052f886f3361bcc0ca94733f\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2024-03-12T16:24:53.318676142Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: true, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; }, { \u0026#34;Subnet\u0026#34;: \u0026#34;fc00:f853:ccd:e793::/64\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;fc00:f853:ccd:e793::1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;006079fd44c64434e818da824356ad71fd80ad05935f88777d1d2d906554ddf5\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;kind-control-plane\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;2da48bb2bbeaefdd98e4749777d7401c84d015e076a4b555dc1686ee1014d63d\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.3/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;fc00:f853:ccd:e793::3/64\u0026#34; }, \u0026#34;6731d2e4298603aa240dab421be84ae0beb1e5c81f27ea59c4097c4844462a82\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;kind-worker\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;d3f13cdea8f66fe3c29c2ff159828d977ef88cf7f2974944c11a3b16e18923d9\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;fc00:f853:ccd:e793::2/64\u0026#34; } }, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.bridge.enable_ip_masquerade\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;Labels\u0026#34;: {} } ] A Linux bridge behaves like a network switch. It forwards packets between interfaces that are connected to it. In the container world, The bridge functions as a virtual networking interface, interconnecting containers (Linux network namespaces) via virtual Ethernet pairs (veth pairs). Each container is configured with its own veth pair, with one end connected to the bridge network. This arrangement enables communication between containers internally and with the host system.\nLinux Namespaces play a pivotal role in containerization by providing isolated environments for individual containers. Each Docker container is encapsulated within its own namespace, ensuring it has its distinct IP address, routing table, and network configuration. This isolation mechanism prevents interference between containers and ensures the host system\u0026rsquo;s integrity. (please check Diving deep into Container Networking: An Exploration of Linux Network Namespace for more details)\nNow, let\u0026rsquo;s attempt to ping one of the node IPs. Remember, in a KinD cluster, nodes are Docker containers. If you\u0026rsquo;re using a Linux-based OS, the ping will be successful. However, if you\u0026rsquo;re on macOS, you\u0026rsquo;ll observe:\nmacbook-pro % ping 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 The ping is timing out!\nContainer\u0026rsquo;s Networking on MacOS # In macOS, Docker-for-Mac doesn\u0026rsquo;t directly expose container networks on the host system. Instead, it operates by running a Linux VM in the background and launches containers within that VM. It enables connections to containers via port binding (L4) and doesn\u0026rsquo;t support connections by IP address (L3).\nLet\u0026rsquo;s explore the routing table in the MacOS host:\nmacbook-pro % netstat -rn Routing tables Internet: Destination Gateway Flags Netif Expire default 192.168.1.1 UGScg en0 default link#17 UCSIg bridge100 ! 10.33.33.2 10.33.33.1 UH utun0 127 127.0.0.1 UCS lo0 127.0.0.1 127.0.0.1 UH lo0 169.254 link#6 UCS en0 ! 192.168.1 link#6 UCS en0 ! 192.168.1.1/32 link#6 UCS en0 ! 192.168.1.1 84:90🅰️3e:a2:4 UHLWIir en0 1126 192.168.1.10/32 link#6 UCS en0 ! 192.168.1.11 80:c:f9:6b:68:84 UHLWI en0 1100 192.168.64 link#17 UC bridge100 ! 192.168.64.1 16.7d.da.9a.15.64 UHLWI lo0 192.168.64.3 6.c5.4b.b4.32.aa UHLWIi bridge100 248 224.0.0/4 link#6 UmCS en0 ! 224.0.0.251 1:0:5e:0:0:fb UHmLWI en0 224.0.0.251 1:0:5e:0:0:fb UHmLWIg bridge100 239.255.255.250 1:0:5e:7f:ff:fa UHmLWI en0 255.255.255.255/32 link#6 UCS en0 ! There\u0026rsquo;s no network configuration to access the Docker network within the Docker Desktop VM. By contrast, comparing this with the Linux host\u0026rsquo;s ip route output reveals significant configuration differences.\nroot@ubuntu # ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.18.0.0/16 dev br-38055463db3c proto kernel scope link src 172.18.0.1 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 Here 172.18.0.0/16 dev br-38055463db3c proto kernel scope link src 172.18.0.1, we can see a route to the kind network via interface br-38055463db3c, which is the bridge interface.\nBy checking the network interfaces in the Linux host we see:\nroot@ubuntu # ip link … 4: br-38055463db3c: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c4:e4:5f:7e brd ff:ff:ff:ff:ff:ff 6: vethf413b4a@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br-38055463db3c state UP mode DEFAULT group default link/ether 0a:86:ac:2a:fe:a5 brd ff:ff:ff:ff:ff:ff link-netnsid 1 … br-38055463db3c: The bridge interface in the host level.\nvethf413b4a@if5: This is a virtual Ethernet interface (veth), it is paired with another veth interface in a linux network namespace of the container. ethf413b4a is connected to the bridge interface br-38055463db3c.\nYou can observe the traffic going in/out of the container by running tcpdump -i vethf413b4a on the host and run curl inside the container to access internet.\nTo achieve similar connectivity on macOS host, we are going to use docker mac net connect. This tool establishes a basic network tunnel between macOS and the Docker Desktop Linux VM. docker-mac-net-connect creates a virtual network interface (utun), acting as the bridge between your Mac and the Docker Desktop Linux VM.\nbrew install chipmk/tap/docker-mac-net-connect sudo brew services start chipmk/tap/docker-mac-net-connect Check the routing table again.\nmacbook-pro % netstat -rn Routing tables Internet: Destination Gateway Flags Netif Expire default 192.168.1.1 UGScg en0 default link#17 UCSIg bridge100 ! 10.33.33.2 10.33.33.1 UH utun0 127 127.0.0.1 UCS lo0 127.0.0.1 127.0.0.1 UH lo0 169.254 link#6 UCS en0 ! 172.17 utun0 USc utun0 172.18 utun0 USc utun0 192.168.1 link#6 UCS en0 ! 192.168.1.1/32 link#6 UCS en0 ! 192.168.1.1 84:90🅰️3e:a2:4 UHLWIir en0 1126 192.168.1.10/32 link#6 UCS en0 ! 192.168.1.11 80:c:f9:6b:68:84 UHLWI en0 1100 192.168.64 link#17 UC bridge100 ! 192.168.64.1 16.7d.da.9a.15.64 UHLWI lo0 192.168.64.3 6.c5.4b.b4.32.aa UHLWIi bridge100 248 224.0.0/4 link#6 UmCS en0 ! 224.0.0.251 1:0:5e:0:0:fb UHmLWI en0 224.0.0.251 1:0:5e:0:0:fb UHmLWIg bridge100 239.255.255.250 1:0:5e:7f:ff:fa UHmLWI en0 255.255.255.255/32 link#6 UCS en0 ! Now, multiple routes have been added to the routing table. 172.18 utun0 USc utun0 indicates that to access the subnet 172.18/16 (the docker network), the traffic should go through the network interface utun0. This network interface is connected on the other side of the tunnel to the Docker VM.\nThe other end of the tunnel (docker VM) is configured by a one-time container with sufficient privileges to configure the Linux host’s network interfaces. The container creates the interface, then exits. Despite the container\u0026rsquo;s termination, the VM interface continues to function because it was created within the Linux host’s network namespace.\nWith these configurations in place, the ping command will function once again, indicating that we now have access to the Docker\u0026rsquo;s network from the macOS host.\nmacbook-pro % ping 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 ... Request timeout for icmp_seq 20 64 bytes from 172.18.0.2: icmp_seq=21 ttl=63 time=3.334 ms 64 bytes from 172.18.0.2: icmp_seq=22 ttl=63 time=1.118 ms 64 bytes from 172.18.0.2: icmp_seq=23 ttl=63 time=1.016 ms Implementing Load Balancer service in KinD cluster with Cilium # To enable north/south traffic in your home lab, deploying a load balancer is a straightforward option. Cilium provides a load balancer feature that allows for load balancing in bare-metal Kubernetes setups. Announcing the load balancer IP to the network is crucial for proper traffic routing, which can be achieved through BGP or L2 routing.\nBGP: BGP stands as a dynamic routing protocol widely utilized in operational Kubernetes environments to broadcast external IP addresses. While its setup may hide greater complexity, BGP brings scalability and resilience through dynamic routing.\nL2 Layer: Alternatively, announcing the LB IP at the L2 layer simplifies the configuration process but may compromise the flexibility and scalability offered by BGP. This method suits smaller, less intricate deployments where simplicity takes precedence.\nWithin our home lab setup, we\u0026rsquo;ll use Cilium\u0026rsquo;s advanced networking features; LB-IPAM alongside L2 announcements. LB-IPAM handles the assignment of IP addresses to services of type LoadBalancer, while L2 announcements ensure services become visible and accessible across the local area network (L2 network).\nNow let\u0026rsquo;s deploy a simple Kubernetes service of type load balancer:\napiVersion: v1 kind: Service metadata: name: sampleservice spec: type: LoadBalancer ports: - port: 80 selector: app: myapp --- apiVersion: apps/v1 kind: Deployment metadata: name: sampleapp spec: selector: matchLabels: app: myapp replicas: 1 template: metadata: labels: app: myapp spec: containers: - name: myserver image: nginx imagePullPolicy: IfNotPresent Check the status of the service:\nmacbook-pro % kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h30m sampleservice LoadBalancer 10.96.28.96 \u0026lt;pending\u0026gt; 80:31508/TCP 4h53m As evident, the service is created with a pending external IP. To initiate IP assignment for the service load balancer, we need to deploy an lb-ipam pool. This pool defines the IP range from which cilium can select an IP address for the service.\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumLoadBalancerIPPool metadata: name: \u0026#34;lb-pool-1\u0026#34; spec: cidrs: - cidr: \u0026#34;172.18.250.0/24\u0026#34; Now let\u0026rsquo;s check the service again:\nmacbook-pro % kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h32m sampleservice LoadBalancer 10.96.28.96 172.18.250.1 80:31508/TCP 4h56m The external IP has now been allocated to the service from the LB-IPAM pool. To implement advanced filtering on the pool, such as service selectors, please consult the documentation here.\nOnce the IP has been assigned, we should be able to broadcast it locally (to all other devices sharing the same physical network L2). To achieve this, we need to create a cilium announcement policy.\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumL2AnnouncementPolicy metadata: name: announcement-policy-1 spec: externalIPs: false loadBalancerIPs: true interfaces: - ^eth[0-9]+ nodeSelector: matchExpressions: - key: node-role.kubernetes.io/control-plane operator: DoesNotExist The \u0026ldquo;interfaces\u0026rdquo; field specifies the network interfaces over which the chosen services will be broadcast. This field is optional, if left unspecified, all interfaces will be utilized. It\u0026rsquo;s essential to note that L2 announcements will function only if the selected devices are included in the \u0026lsquo;devices\u0026rsquo; set specified in the field \u0026lsquo;devices=\u0026quot;{eth0,net0}\u0026quot;\u0026rsquo;.\nL2 announcement functions are based on the Address Resolution Protocol (ARP). ARP is a fundamental protocol in computer networks used to map IP addresses to MAC addresses. Here\u0026rsquo;s a breakdown of ARP\u0026rsquo;s operation when trying to access Kubernetes load balancer service “curl 172.18.250.1/” from the local computer:\nARP Process Begins: The local computer checks its ARP cache to see if it has the MAC address of the destination IP address (docker container). If not found, an ARP request process begins.\nARP Request Packet Creation: The local computer constructs an ARP request packet containing the IP address of the destination (the LoadBalancer IP) and its own MAC address. The destination MAC address in the ARP request packet is set to the broadcast MAC address (FF:FF:FF:FF:FF:FF) to ensure all devices on the local network receive it.\nARP Request Broadcast: The local computer sends the ARP request packet as a broadcast frame onto the local network segment (L2 network). All devices on the local network receive this ARP request.\nCilium Agent Responds: after receiving the ARP request by the Kubernetes nodes, the Cilium agent checks if it has the specified IP address. If found, it responds with an ARP reply packet containing the MAC address of the node (in this case, the Docker container\u0026rsquo;s IP).\nARP Reply Packet: The ARP reply packet is sent directly to the MAC address of the local computer that initiated the ARP request.\nARP Cache Update: The local computer receives the ARP reply packet, extracts the MAC address of the LoadBalancer, and updates its ARP cache with this mapping. This mapping is cached for future use to avoid sending ARP requests for the same IP address.\nThe ARP table contains only the most recent MAC address associated with an IP. As a result, only one node in the cluster can reply to requests for a specific IP address. To ensure this, each Cilium agent selects services for its node and participates in leader election using Kubernetes leases. Every service corresponds to a lease, and the lease holder is responsible for responding to requests on designated interfaces.\nkubectl get leases -n kube-system NAME HOLDER AGE … cilium-l2announce-default-sampleservice kind-worker 4m2s … Now notice that the lease cilium-l2announce-default-sampleservice has elected kind-worker as a leader, in this case the cilium agent in this node will help in the ARP resolution process. Let\u0026rsquo;s capture the ARP traffic arriving at the kind-worker node using tcpdump.\nCILIUM_POD=$(kubectl -n kube-system get pod -l k8s-app=cilium --field-selector spec.nodeName=kind-worker -o name) kubectl -n kube-system exec -ti $CILIUM_POD -- bash apt-get update \u0026amp;\u0026amp; DEBIAN_FRONTEND=noninteractive apt-get -y install tcpdump termshark tcpdump -i any arp -w arp.pcap Now, open another terminal and run curl command with the Load Balancer IP address.\ncurl 172.18.250.1/ You\u0026rsquo;ll notice that the curl request is successful. Go back now to the Cilium agent terminal and stop the running command. Next, open Termshark and load the ARP file to examine the results.\nTERM=xterm-256color termshark -r arp.pcap In both screenshots we can see the ARP request and reply details provide insights into how network devices communicate and resolve MAC addresses to IP addresses. Here\u0026rsquo;s an explanation of each component:\nARP Request (1st screenshot): Sender\u0026rsquo;s IP Address: The IP address of the device sending the ARP request. Sender\u0026rsquo;s MAC Address: The MAC address of the device sending the ARP request. Target IP Address: The IP address for which the MAC address is being requested. Target MAC Address: This field is typically set to all zeros in ARP requests since the sender is asking for the MAC address associated with a specific IP address. It\u0026rsquo;s often referred to as the \u0026ldquo;broadcast MAC address\u0026rdquo; (FF:FF:FF:FF:FF:FF).\nARP Reply (2nd screenshot): Sender\u0026rsquo;s IP Address: The IP address of the device sending the ARP reply. Sender\u0026rsquo;s MAC Address: The MAC address of the device sending the ARP reply. Target IP Address: The IP address for which the MAC address is being provided. Target MAC Address: The MAC address associated with the target IP address, as requested in the ARP request.\nConclusion # Setting up the networking and load balancing in a KinD cluster with Cilium involves careful configuration and troubleshooting. By understanding the underlying principles and employing the right tools and techniques, you can ensure smooth operation and optimal performance of your home lab environment.\nYou\u0026rsquo;ll discover a script containing all the commands demonstrated in this article here. Also, consider using codespaces to run the lab.\n","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/103/articles/networking/setting-up-load-balancer-service-with-cilium-in-kind-cluster/","section":"Articles","summary":"Kubernetes in Docker (KinD) offers a lightweight and efficient way to run Kubernetes clusters for development and testing purposes. However, setting up KinD with load balancing option requires specific networking configurations. In this article, we\u0026rsquo;ll explore the networking configuration of KinD on both Linux and MacOS, deep dive into load balancing options and discuss troubleshooting tactics.\nRequirements # Docker KinD Kubectl Ciliumctl Setting Up Kubernetes in Docker # To create a KinD cluster with two nodes, you can use the following configuration:","title":"Setting up Load Balancer Service with Cilium in KinD Cluster","type":"articles"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/103/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/103/categories/dev/","section":"Categories","summary":"","title":"Dev","type":"categories"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/103/categories/frontend/","section":"Categories","summary":"","title":"Frontend","type":"categories"},{"content":"The first version of the homepage is up \u0026#x1f389;!\nCurrent tools # mkdocs mkdocs-material theme took a lot of inspiration from the blowfish Hugo theme Mixed feeling # While this worked reasonably well for the homepage, my feeling is that it required a lot of custom code and I spent way too much time writing HTML and CSS. This is probably not what we want to do when adding content, but instead, work with Markdown as much as possible.\nAt this point, I wonder if Hugo would have been a better choice, mkdocs is heavily focused on documentation websites, which is not the purpose of what we are trying to build \u0026#x1f914;\n","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/103/blog/posts/frontend-adventures/level-1/","section":"Blog","summary":"The first version of the homepage is up \u0026#x1f389;!\nCurrent tools # mkdocs mkdocs-material theme took a lot of inspiration from the blowfish Hugo theme Mixed feeling # While this worked reasonably well for the homepage, my feeling is that it required a lot of custom code and I spent way too much time writing HTML and CSS. This is probably not what we want to do when adding content, but instead, work with Markdown as much as possible.","title":"Frontend adventures - Level 1","type":"blog"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/articles/","section":"Articles","summary":"","title":"Articles","type":"articles"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"Charles-Edouard Brétéché is a Staff Engineer at Nirmata, a maintainer for Kyverno, and has created and contributed to various open source projects. including a Terraform provider for kOps.\nHe has been building and delivering software for more than 20 years, as a software engineer, SRE, platform engineer, devops engineer and software architect.\n","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/authors/eddycharly/","section":"Authors","summary":"Charles-Edouard Brétéché is a Staff Engineer at Nirmata, a maintainer for Kyverno, and has created and contributed to various open source projects. including a Terraform provider for kOps.\nHe has been building and delivering software for more than 20 years, as a software engineer, SRE, platform engineer, devops engineer and software architect.","title":"Charles-Edouard Brétéché","type":"authors"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/podcasts/","section":"Podcasts","summary":"","title":"Podcasts","type":"podcasts"},{"content":"Sara Qasmi, a freelance platform engineer, is driven by her passion for constructing secure and reliable Kubernetes platforms.\nWith a varied background spanning software engineering and DevOps, she is deeply engaged in advancing modern platforms that empower businesses to innovate and scale seamlessly.\n","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/authors/sara/","section":"Authors","summary":"Sara Qasmi, a freelance platform engineer, is driven by her passion for constructing secure and reliable Kubernetes platforms.\nWith a varied background spanning software engineering and DevOps, she is deeply engaged in advancing modern platforms that empower businesses to innovate and scale seamlessly.","title":"Sara Qasmi","type":"authors"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/103/videos/","section":"Videos","summary":"","title":"Videos","type":"videos"}]