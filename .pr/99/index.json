[{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/articles/","section":"Articles","summary":"","title":"Articles","type":"articles"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/bridge/","section":"Tags","summary":"","title":"Bridge","type":"tags"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/container/","section":"Tags","summary":"","title":"Container","type":"tags"},{"content":" Introduction # Container networking might seem complex and almost mystical initially, but it\u0026rsquo;s actually built on basic Linux networking principles. By understanding these fundamentals, we can troubleshoot the container networking layers on a profound level. Furthermore, we might even create container networking solution from scratch for pure enjoyment.\nIn this article we are going to cover the foundational elements of container networking, from the underlying principles of network namespaces to the practical tools and techniques for managing container networking environments.\nRequirements # For this article, you\u0026rsquo;ll require a Linux host system equipped with the following utilities: nsenter, ping, and tcpdump.\nDive Into The Foundations of Container Networking # A container is essentially, an isolated and restricted Linux process within its own networking context that is completely separate from both the host system and other containers.\nAt the heart of this isolation lies a fascinating Linux kernel feature known as network namespaces. According to the man page, \u0026ldquo;network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.\u0026rdquo;.\nEssentially, it allows processes to operate within their own distinct network environment, including having their own network interfaces, routing tables, and firewall configurations.\nThis capability of Linux kernel empowers containerization technologies like Docker. By placing containers within their own network namespaces, they gain autonomy over their network configurations and ensures that containers can communicate with each other or the external network without causing interference with other containers or the host system\u0026rsquo;s network setup (root namespace).\nTo unravel this concept, let\u0026rsquo;s delve into some fundamental aspects.\nWhat constitutes a Networking Stack? # The networking stack is a set of software layers designed to facilitate communication across networks. Each layer within this stack is responsible for specific tasks such as data transmission, addressing, routing, and interaction with applications. Together, these layers collaborate to establish communication between devices connected to a network. This networking stack typically follows the OSI (Open Systems Interconnection) model.\nHere\u0026rsquo;s a overview of the layers commonly present in a networking stack:\nEach layer of the networking stack interacts with the layers above and below it, encapsulating data as it moves down the stack and decapsulating it as it moves up. In this article, we\u0026rsquo;ll relay on the key components of the network stack, including:\nNetwork Devices: Network devices, such as network interface cards (NICs), switches, and routers, are physical or virtual entities responsible for transmitting and receiving data packets on a network. These devices interface with the Linux networking stack to exchange data with other devices. You can list these devices using the command ip link list.\nRouting Table: The routing table is a critical component of the networking stack that contains information about the available routes to different network destinations. When a packet arrives at a Linux system, the networking stack consults the routing table to determine the appropriate path for forwarding the packet to its destination. Viewable with ip route list.\niptables Rules: iptables is a powerful firewall utility in Linux that allows administrators to define rules for packet filtering and network address translation (NAT). These rules are organized into chains within the iptables framework.\nThe sequence of rules are applied to packets as they traverse the networking stack of a Linux system. These rules dictate how packets are processed, filtered, and potentially modified as they move through various stages of the networking stack.\nThere are three main built-in chains in iptables, each corresponding to a different stage of packet processing:\nINPUT: Used for packets destined for the local system. FORWARD: Used for packets passing through the system. OUTPUT: Used for packets originating from the local system. When a packet reaches a chain, it is compared against the rules defined in that chain. Each rule specifies criteria that the packet must match (such as source or destination IP address, port number, protocol, etc.), as well as an action to take if the packet matches the criteria (such as accept, drop, reject, or forward to another chain).\nIn addition to these three built-in chains, users can also create custom chains to organize rules more efficiently or to perform specific tasks. iptables --list-rules.\nWait of our next article for more details on iptables.\nInspect the Network Environment # Now let\u0026rsquo;s apply what we learned before and inspect the network environment before running containers.\nNetwork devices:\nroot@ubuntu:~# ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff Routing table:\nroot@ubuntu:~# ip route list default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 iptable rules:\nroot@ubuntu:~# iptables --list-rules -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER ... Note: If Docker is running on your host system, you will notice some custom chains added to your iptables rules.\nCreate A Network Namespace # Linux offers various tools and utilities for network namespace management. The ip netns command facilitates the creation, deletion, and the configuration of network namespaces.\nCreate a new network namespace:\nroot@ubuntu:~# ip netns add app1 To display the network namespaces that have been created, you can use the following command:\nroot@ubuntu:~# ip netns list app1 To execute a process within a network namespace, you can utilize the nsenter utility. Here\u0026rsquo;s how you can initiate a new shell within the app1 namespace:\nroot@ubuntu:~# nsenter --net=/run/netns/app1 bash Now that we\u0026rsquo;ve created a Bash process in the app1 namespace, we can execute our commands within the network namespace.\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 root@ubuntu:~# ip route list root@ubuntu:~# iptables --list-rules -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT This output indicates that the Bash process is operating within a fully isolated namespace (app1). There are no routing rules or custom iptables chains present, and only the loopback interface is available.\nNote: loopback interface is a virtual network interface that allows communication between applications running on the same network namespace.\nNetwork Namespace to Host connectivity # Remember, network namespaces are isolated from each other and from the host. Now, let\u0026rsquo;s establish a connection between the container\u0026rsquo;s network namespace app1 and the host.\nTo accomplish this, we\u0026rsquo;ll use virtual Ethernet devices (veth). A veth device is a type of virtual network interface in Linux, creating a virtual network link between two network namespaces or between a network namespace and the host system.\nHere\u0026rsquo;s how it works:\nA veth device is typically configured as a pair of virtual network interfaces. One end of the pair resides within the container\u0026rsquo;s network namespace, while the other end is situated either in another network namespace or directly within the host system.\nOpen a new shell session to access the root network namespace:\nroot@ubuntu:~# ip link add hlink1 type veth peer name clink1 This command creates a pair of virtual Ethernet interfaces veth, named hlink1 and clink1, within the root network namespace. You can observe the outcome by listing the network devices using the following command.\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff ... 9: clink1@hlink1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff 10: hlink1@clink1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff The output shows that both hlink1 and clink1 interfaces are located within the root network namespace and the state of each one is DOWN.\nTo establish a connection between the root network namespace and the app1 network namespace, we must retain one of the interfaces, hlink1, in the root namespace while relocating the other one, clink1, into the app1 namespace (use the following command).\nroot@ubuntu:~# ip link set clink1 netns app1 Again, list the network interfaces in the root network namespace:\nroot@ubuntu:~# ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 06:c5:4b:b4:32:aa brd ff:ff:ff:ff:ff:ff ... 10: hlink1@if9: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff link-netns app1 The output shows that the clink1 interface has been removed from the network device list within the root network namespace.\nNow, check the network device list in app1 namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip link list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: clink1@if10: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 The clink1 interface has been moved from the root network namespace to app1. Notice the inclusion of link-netns app1 in the hlink1 interface within the root network namespace\u0026rsquo;s device list output. This indicates that the hlink1 interface is now linked with the app1 network namespace.\nNote: keep in mind that both interfaces, clink1 and lo, in app1 network namespace are DOWN.\nCurrently, each end of the veth pair resides in its respective namespace to establish the connection between both network namespaces. However, the link cannot yet be utilized for communication between the namespaces until we assign suitable IP addresses to the different interfaces and activate them.\nLet\u0026rsquo;s assigne 172.16.0.11/16 to clink1 within app1 network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 bash -c \u0026#39; ip link set clink1 up ip addr add 172.16.0.11/16 dev clink1 \u0026#39; Review the configuration of clink1 within the app1 network namespace to inspect the assignment of IP addresses.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip addr show dev clink1 9: clink1@if10: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000 link/ether a2:08:37:6f:7a:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.16.0.11/16 scope global clink1 valid_lft forever preferred_lft forever Additionally, we must setup the loopback interface within the app1 network namespace. This ensures seamless communication among processes within the network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip link set lo up Now, returning to the root network namespace, let\u0026rsquo;s assign the IP address 172.16.0.10/16 to the hlink1 interface.\nroot@ubuntu:~# ip link set hlink1 up root@ubuntu:~# ip addr add 172.16.0.10/16 dev hlink1 Inspect the hlink1 configuration within the root network namespace to see the IP assignment.\nroot@ubuntu:~# ip addr show dev hlink1 10: hlink1@if9: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b2:2d:19:e8:a0:28 brd ff:ff:ff:ff:ff:ff link-netns app1 inet 172.16.0.10/16 scope global hlink1 valid_lft forever preferred_lft forever We\u0026rsquo;ve successfully established the link between the app1 network namespace and the root, as illustrated in the diagram above. Now, let\u0026rsquo;s inspect the routing table within the app1 and root network namespace to understand how traffic originating from app1 namespace will be routed to reach the root.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ip route 172.16.0.0/16 dev clink1 proto kernel scope link src 172.16.0.11 The output shows that traffic distinated for 172.16.0.0/16 will be routed through the clink1 interface. Given that clink1 is one end of the veth pair, it implies that any traffic traversing this interface will also be observable on the other end of the link hlink1 within the root namespace.\nThen the routing table of the root network namespace.\nroot@ubuntu:~# ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 The route 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 indicates that traffic destined for 172.16.0.0/16 will be routed through the hlink1 interface. Since hlink1 is one end of the veth pair, it also means that any traffic passing through this interface will also be visible on the other end of the link, clink1, within the app1 network namespace.\nLet\u0026rsquo;s start our first connectivity test by pinging hlink1 from the app1 network namespace.\nroot@ubuntu:~# nsenter --net=/run/netns/app1 ping 172.16.0.10 PING 172.16.0.10 (172.16.0.10) 56(84) bytes of data. 64 bytes from 172.16.0.10: icmp_seq=1 ttl=64 time=0.224 ms 64 bytes from 172.16.0.10: icmp_seq=2 ttl=64 time=0.045 ms And now, clink1 from the root network namespace.\nroot@ubuntu:~# ping 172.16.0.11 PING 172.16.0.11 (172.16.0.11) 56(84) bytes of data. 64 bytes from 172.16.0.11: icmp_seq=1 ttl=64 time=0.047 ms 64 bytes from 172.16.0.11: icmp_seq=2 ttl=64 time=0.060 ms The connectivity test has been successful between root network namespace and app1.\nNetwork Namespace to Network Namespace connectivity # In this section, we\u0026rsquo;ll demonstrate that relying only on veth pairs isn\u0026rsquo;t enough to interconnect two or more containers. This leads to conflicts in the Layer 3 (L3) routing table caused by introducing separate rules for different network namespaces in root namespace routing table. Let\u0026rsquo;s create a second network namespace app2 and configure the veth pair as in the app1:\nip netns add app2 ip link add hlink2 type veth peer name clink2 ip link set hlink2 up ip addr add 172.16.0.20/16 dev hlink2 ip link set clink2 netns app2 nsenter --net=/run/netns/app2 bash -c \u0026#39; ip link set lo up ip link set clink2 up ip addr add 172.16.0.21/16 dev clink2 exit \u0026#39; Now, check the routing table of the root network namespace:\nroot@ubuntu:~# ip route ... 172.16.0.0/16 dev hlink1 proto kernel scope link src 172.16.0.10 # app1 172.16.0.0/16 dev hlink2 proto kernel scope link src 172.16.0.20 # app2 ... As you can see, the root network namespace\u0026rsquo;s routing table encountered a conflict. After adding the second veth pair, a new route was added: 172.16.0.0/16 dev hlink2 proto kernel scope link src 172.16.0.20. However, there was already an existing route for the 172.16.0.0/16 network. Consequently, when app2 tries to ping the hlink2 device, the first route is chosen, leading to connectivity issues.\nConnectivity test again.\nroot@ubuntu:~# nsenter --net=/run/netns/app2 ping 172.16.0.20 # ping root\u0026#39;s interface hlink2 PING 172.16.0.20 (172.16.0.20) 56(84) bytes of data. --- 172.16.0.20 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 1023ms As expected, the connectivity test confirmes that accessing hlink2 from app2 is not feasible, even with the routes present in routing tables.\nAlternatively, to confirm this, we can use tcpdump on the hlink2 interface within the root network namespace. By running the command tcpdump -i hlink2 icmp, all ICMP packets passing through the hlink2 interface are captured. However, in this scenario, no traffic will be captured.\nLinux Bridge # The previous issue pushed us to explore alternatives for interconnecting containers, leading us to the Linux feature known as Linux Bridge. Operating similarly to a network switch, a Linux bridge facilitates packet forwarding between connected interfaces at the L2 level. (Please check Setting up Load Balancer Service with Cilium in KinD Cluster for more details on l2 connectivity)\nIn the container ecosystem, the bridge acts as a virtual networking interface, linking container\u0026rsquo;s network namespace through virtual Ethernet pairs. Each network namespace is equipped with its own veth pair, with one end attached to the bridge network. This setup facilitates internal communication between containers and with the host system.\nIn this section, we will connect two new network namespaces to each other using a bridge. To begin, create two new network namespaces named app3 and app4.\nip netns add app3 ip link add hlink3 type veth peer name clink3 ip link set hlink3 up ip link set clink3 netns app3 nsenter --net=/run/netns/app3 bash -c \u0026#39; ip link set lo up ip link set clink3 up ip addr add 172.16.0.30/16 dev clink3 ip link \u0026#39; ip netns add app4 ip link add hlink4 type veth peer name clink4 ip link set hlink4 up ip link set clink4 netns app4 ip link nsenter --net=/run/netns/app4 bash -c \u0026#39; ip link set lo up ip link set clink4 up ip addr add 172.16.0.40/16 dev clink4 ip link \u0026#39; Make sure that there is no new routes added to the routing table of the root namespace:\nroot@ubuntu:~# ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 Now, let\u0026rsquo;s create the bridge device and connect hlink3 and hlink4 to it to ensure connectivity between the containers.\nip link add br0 type bridge ip link set br0 up ip link set hlink3 master br0 ip link set hlink4 master br0 ip link Connectivity Test # Note: If you are running docker in your host, make sure to remove br_netfilter kernel module rmmod br_netfilter (for more details https://unix.stackexchange.com/questions/671644/cant-establish-communication-between-two-network-namespaces-using-bridge-networ)\nBefore the ping test, ensure to run tcpdump on the bridge interface to capture the ICMP traffic between both containers.\ntcpdump -i br0 icmp Now, switch to another shell session and execute the following commands:\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ping -c 2 172.16.0.40 # ping app4 PING 172.16.0.40 (172.16.0.40) 56(84) bytes of data. 64 bytes from 172.16.0.40: icmp_seq=1 ttl=64 time=0.142 ms 64 bytes from 172.16.0.40: icmp_seq=2 ttl=64 time=0.071 ms --- 172.16.0.40 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1025ms rtt min/avg/max/mdev = 0.071/0.106/0.142/0.035 ms root@ubuntu:~# nsenter --net=/run/netns/app4 ping -c 2 172.16.0.30 # ping app3 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. 64 bytes from 172.16.0.30: icmp_seq=1 ttl=64 time=0.053 ms 64 bytes from 172.16.0.30: icmp_seq=2 ttl=64 time=0.057 ms --- 172.16.0.30 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1028ms rtt min/avg/max/mdev = 0.053/0.055/0.057/0.002 ms We can see from the tcpdum command that the ICMP traffic if going through brigde interface to reach each destination network namespace. Here is the output:\nroot@ubuntu:~# tcpdump -i br0 icmp tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on br0, link-type EN10MB (Ethernet), snapshot length 262144 bytes 16:19:13.037613 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo request, id 44793, seq 1, length 64 16:19:13.037626 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo reply, id 44793, seq 1, length 64 16:19:14.062746 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo request, id 44793, seq 2, length 64 16:19:14.062781 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo reply, id 44793, seq 2, length 64 16:19:42.217807 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo request, id 13480, seq 1, length 64 16:19:42.217839 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo reply, id 13480, seq 1, length 64 16:19:43.245866 IP 172.16.0.40 \u0026gt; 172.16.0.30: ICMP echo request, id 13480, seq 2, length 64 16:19:43.245894 IP 172.16.0.30 \u0026gt; 172.16.0.40: ICMP echo reply, id 13480, seq 2, length 64 The test is a success! Without configuring any IP addresses for hlink3 and hlink4; instead, we assigned IPs only to clink3 and clink4. As hlink3 and hlink4 belong to the same Ethernet segment (connected to the bridge), L2 connectivity is established based on MAC addresses. This is further confirmed by inspecting the ARP table in both namespaces.\nARP (Address Resolution Protocol) table in Linux offers a mapping between IP addresses and MAC addresses of devices on the local network segment (L2 segment).\nroot@ubuntu:~# nsenter --net=/run/netns/app3 arp -n Address HWtype HWaddress Flags Mask Iface 172.16.0.40 ether 52:45:d1:3a:b3:32 C clink3 Here, we can see that to reach app4 from app3, the L2 frame will use the destination MAC address 52:45:d1:3a:b3:32 for the IP destination address 172.16.0.40.\nSame thing in app4\nroot@ubuntu:~# nsenter --net=/run/netns/app4 arp -n Address HWtype HWaddress Flags Mask Iface 172.16.0.30 ether 82:6e:c9:b4:c0:79 C clink4 Note: To get the MAC address of each network interface, execute ip addr show INTF_NAME.\nAlternatively, the ip neigh command offers information about the neighbor cache.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ip neigh 172.16.0.40 dev clink3 lladdr 52:45:d1:3a:b3:32 REACHABLE In this specific output, we observe that app3 has a neighbor device with the IP address 172.16.0.40 (app4), accessible through the clink3 interface. The MAC address (Link Layer address) of this neighbor device is identified as 52:45:d1:3a:b3:32. Furthermore, the state of this entry in the neighbor cache is marked as REACHABLE, indicating that the MAC address is presently known and accessible.\nWhen two devices within the same L2 segment, such as app3 and app4 connected via the bridge, need to communicate, they rely on MAC addresses. The ARP table entries play a crucial role in this process by actively providing the necessary MAC destination address for constructing the L2 frame.\nAfter establishing the connection between app3 and app4, it\u0026rsquo;s now time to enssure the connectivity between the host (root network namespace) and both app3 and app4. First, let\u0026rsquo;s attempt to ping app3 from the root network namespace.\nroot@ubuntu:~# ping -c 2 172.16.0.30 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. From 172.16.0.10 icmp_seq=1 Destination Host Unreachable From 172.16.0.10 icmp_seq=2 Destination Host Unreachable --- 172.16.0.30 ping statistics --- 2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 1017ms The root namespace is currently unable to communicate with the app3 network namespace because there\u0026rsquo;s no route in root\u0026rsquo;s routing table allowing to reach app3. To this, we first need to assign an IP address to the bridge network interface.\nip addr add 172.16.0.1/16 dev br0 Assigning an IP address to br0 updates the root namespace routing table with the route: 172.16.0.0/16 dev br0 proto kernel scope link src 172.16.0.1. This indicates that traffic destined for the 172.16.0.0/16 subnet can be routed via the br0 interface. Given that hlink1 and hlink2 are also connected to the bridge, this implies that traffic will be appropriately directed to the target namespaces.\nRemember, the bridge operates at the data link layer (L2), so once traffic is routed to the bridge interface, an ARP request is broadcasted to all devices connected to the bridge, requesting the destination MAC address. The device that possesses the MAC address will respond, and the Ethernet frame will be filled with the destination MAC address before being sent to its destination within this L2 segment.\nAfter this configuration, we should be able to ping app3 and app4 from the root network namespace.\nroot@ubuntu:~# ping -c 2 172.16.0.30 PING 172.16.0.30 (172.16.0.30) 56(84) bytes of data. 64 bytes from 172.16.0.30: icmp_seq=1 ttl=64 time=0.085 ms 64 bytes from 172.16.0.30: icmp_seq=2 ttl=64 time=0.069 ms --- 172.16.0.30 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1016ms rtt min/avg/max/mdev = 0.069/0.077/0.085/0.008 ms We should also add a default route to app3 and app4. This means that if there is no destination subnet specified, the traffic will be sent to the default gateway, which in this case is the bridge interface.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ip route add default via 172.16.0.1 root@ubuntu:~# nsenter --net=/run/netns/app4 ip route add default via 172.16.0.1 Now, app3 and app4 can reach any subnet within the host.\nroot@ubuntu:~# nsenter --net=/run/netns/app3 ping -c 2 192.168.64.3 PING 192.168.64.3 (192.168.64.3) 56(84) bytes of data. 64 bytes from 192.168.64.3: icmp_seq=1 ttl=64 time=0.184 ms 64 bytes from 192.168.64.3: icmp_seq=2 ttl=64 time=0.066 ms --- 192.168.64.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1019ms rtt min/avg/max/mdev = 0.066/0.125/0.184/0.059 ms Great! We can go from the containers to the host and back.\nConclusion # Understanding Linux networking features is fundamental to many containerization technologies. This foundational knowledge serves as a basis for comprehending higher-level networking concepts used in Kubernetes. In our upcoming article, we\u0026rsquo;ll explore how Container Runtime Interface (CRI) and Container Network Interface (CNI) leverage these features within the Kubernetes ecosystem.\n","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/articles/networking/diving-deep-into-container-networking/","section":"Articles","summary":"Introduction # Container networking might seem complex and almost mystical initially, but it\u0026rsquo;s actually built on basic Linux networking principles. By understanding these fundamentals, we can troubleshoot the container networking layers on a profound level. Furthermore, we might even create container networking solution from scratch for pure enjoyment.\nIn this article we are going to cover the foundational elements of container networking, from the underlying principles of network namespaces to the practical tools and techniques for managing container networking environments.","title":"Diving deep into Container Networking (An Exploration of Linux Network Namespace)","type":"articles"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/","section":"Fence","summary":"","title":"Fence","type":"page"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/series/networking/","section":"Series","summary":"","title":"Networking","type":"series"},{"content":"Sara Qasmi, a freelance platform engineer, is driven by her passion for constructing secure and reliable Kubernetes platforms.\nWith a varied background spanning software engineering and DevOps, she is deeply engaged in advancing modern platforms that empower businesses to innovate and scale seamlessly.\n","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/authors/sara/","section":"Authors","summary":"Sara Qasmi, a freelance platform engineer, is driven by her passion for constructing secure and reliable Kubernetes platforms.\nWith a varied background spanning software engineering and DevOps, she is deeply engaged in advancing modern platforms that empower businesses to innovate and scale seamlessly.","title":"Sara Qasmi","type":"authors"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"May 9, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Charles-Edouard Brétéché is a Staff Engineer at Nirmata, a maintainer for Kyverno, and has created and contributed to various open source projects. including a Terraform provider for kOps.\nHe has been building and delivering software for more than 20 years, as a software engineer, SRE, platform engineer, devops engineer and software architect.\n","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/authors/eddycharly/","section":"Authors","summary":"Charles-Edouard Brétéché is a Staff Engineer at Nirmata, a maintainer for Kyverno, and has created and contributed to various open source projects. including a Terraform provider for kOps.\nHe has been building and delivering software for more than 20 years, as a software engineer, SRE, platform engineer, devops engineer and software architect.","title":"Charles-Edouard Brétéché","type":"authors"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/ci/","section":"Tags","summary":"","title":"CI","type":"tags"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/series/offline-kubernetes-validation/","section":"Series","summary":"","title":"Offline Kubernetes Validation","type":"series"},{"content":"In this article, we will explore kubeconform, the successor to kubeval.\nKubeconform is based on the same principles as kubeval, but it is actively maintained and has some unique features.\nIntroduction # As the two tools are very similar, I invite you to consult the article on kubeval which also applies to kubeconform.\nResource Schemas # Kubeconform uses JSON schemas to validate (or invalidate) the proposed resources. These JSON schemas are created from the OpenAPI schemas published within the Kubernetes GitHub repository.\nUnlike kubeval, which is no longer maintained, kubeconform maintains an up-to-date repository of JSON schemas converted from the native schemas published by Kubernetes (kubeconform fork of kubernetes-json-schema). The most recent version supported as of today is v1.30.0.\nInstalling kubeconform # The kubeconform project documentation suggests installing the binary directly or using a package manager, or using it through a Docker image.\nFor this article, we will use brew to install it locally:\nbrew install kubeconform The docker image can be found at https://ghcr.io/yannh/kubeconform.\nValidation with kubeconform # Unlike kubeval, kubeconform supports the validation of custom resources, provided that you supply the corresponding JSON schemas. We will see more about this later.\nLet’s start by validating native resources.\nValidating native resources # Let\u0026rsquo;s try to validate the simple (yet invalid) manifest below:\napiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 template: spec: containers: - image: nginx name: nginx If we run kubeconform without any option the DaemonSet will be considered invalid:\nkubeconform manifests.yaml manifests.yaml - DaemonSet nginx-ds is invalid: problem validating schema. Check JSON formatting: jsonschema: \u0026#39;/spec\u0026#39; does not validate with https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master-standalone/daemonset-apps-v1.json#/properties/spec/required: missing properties: \u0026#39;selector\u0026#39; As noted above, the selector field is missing. Let\u0026rsquo;s provide a valid manifest:\napiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 selector: matchLabels: name: nginx-ds template: metadata: labels: name: nginx-ds spec: containers: - image: nginx name: nginx This time, running kubeconform manifests.yaml passes. Still, the unexpected replicas field is not detected.\nTo make kubeconform detect unexpected fields, we need to specify --strict:\nkubeconform --strict manifests.yaml manifests.yaml - DaemonSet nginx-ds is invalid: problem validating schema. Check JSON formatting: jsonschema: \u0026#39;/spec\u0026#39; does not validate with https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/master-standalone-strict/daemonset-apps-v1.json#/properties/spec/additionalProperties: additionalProperties \u0026#39;replicas\u0026#39; not allowed Corner cases # As explained in the article on kubeval, schema-based validation is often approximate.\nProperty type checks work quite well, but the validation of associated values is less reliable. This is because the validation logic is implemented in specific code and does not always form part of the schema definition.\nLet\u0026rsquo;s take the example of an invalid label:\napiVersion: v1 kind: ConfigMap metadata: name: cm labels: # ERROR: a label key must start with an alphanumeric character _: invalid data: foo: bar Let\u0026rsquo;s try to validate the resource above with:\nkubeconform --strict manifests.yaml This time kubeconform does not detect the invalid label.\nAnother simple example is the following, which defines a Deployment with a negative number of replicas:\napiVersion: apps/v1 kind: Deployment metadata: name: deploy spec: replicas: -1 selector: matchLabels: name: deploy template: metadata: labels: name: deploy spec: containers: - image: nginx name: nginx Here too, kubeconform --strict manifests.yaml does not detect the problem even though the resource will be rejected by a cluster:\nkubectl apply -f manifests.yaml The Deployment \u0026#34;deploy\u0026#34; is invalid: spec.replicas: Invalid value: -1: must be greater than or equal to 0 Target Kubernetes Version # In the case of native resources, the target version of the cluster is important, as native resources evolve regularly with the appearance of new properties, new API versions, and some API versions are deprecated and then removed, etc.\nTherefore, a resource considered valid given one version of Kubernetes may be considered invalid with an older version of Kubernetes.\nIt is possible to target a specific version of Kubernetes using the --kubernetes-version argument.\nLet\u0026rsquo;s take, for example, the API extensions/v1beta1 which was removed from Kubernetes in v1.22:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: frontend spec: rules: - host: frontend.minikube.local http: paths: - path: / backend: serviceName: frontend servicePort: 80 The Ingress resource above is considered valid with Kubernetes v1.20:\nkubeconform --kubernetes-version 1.20.0 --strict manifests.yaml While the schema is no longer available in v1.22:\nkubeconform --kubernetes-version 1.22.0 --strict manifests.yaml manifests.yaml - Ingress frontend failed validation: could not find schema for Ingress CRDs validation # Let\u0026rsquo;s finish with the validation of custom resources (CRDs).\nKubeconform supports custom resources provided that you can supply it with the corresponding JSON schema.\nFor instance, using an example from kubeconform\u0026rsquo;s GitHub, we can utilize Datree\u0026rsquo;s CRDs-catalog which lists the most used CRDs and provides the associated JSON schemas.\nLet\u0026rsquo;s take for example an Issuer resource of cert manager:\napiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} Without specifying the location of the schema associated with the Issuer resource type, kubeconform is unable to validate the resource:\nkubeconform --strict manifests.yaml manifests.yaml - Issuer test-selfsigned failed validation: could not find schema for Issuer In order to enable kubeconform to download the corresponding JSON schema, one can use the --schema-location argument:\nkubeconform --schema-location \u0026#39;https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json\u0026#39; --strict manifests.yaml This time, kubeconform can validate the Issuer resource using the schema downloaded from Datree\u0026rsquo;s GitHub repository.\nNote that the --schema-location argument can be specified as many times as necessary. It is also possible to convert a CRD definition yourself, which is all explained here.\nConclusion # In conclusion, kubeconform is heavily inspired by kubeval and has taken up the mantle to become its worthy successor.\nBoth tools operate in exactly the same way based on JSON schema, and kubeconform maintains an up-to-date version of the schemas for native resources.\nFinally, kubeconform adds support for custom resources, which significantly broadens the tool\u0026rsquo;s scope. Their use, however, remains quite complex, especially in the case of a custom resource whose schema is not publicly available on the internet.\nIn summary, kubeconform is a significant step forward for users of kubeval, providing a satisfactory tool for use without custom resources. However, it remains limited in the case of intensive use of CRDs.\nStay tuned! # The next article will explore a tool more suited to modern Kubernetes usage with enhanced support for custom resources. Stay tuned for the next episode!\n","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/articles/k8s-resources-validation/kubeconform-review/","section":"Articles","summary":"In this article, we will explore kubeconform, the successor to kubeval.\nKubeconform is based on the same principles as kubeval, but it is actively maintained and has some unique features.\nIntroduction # As the two tools are very similar, I invite you to consult the article on kubeval which also applies to kubeconform.\nResource Schemas # Kubeconform uses JSON schemas to validate (or invalidate) the proposed resources. These JSON schemas are created from the OpenAPI schemas published within the Kubernetes GitHub repository.","title":"Validating Kubernetes resources offline - kubeconform review","type":"articles"},{"content":"","date":"May 6, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/validation/","section":"Tags","summary":"","title":"Validation","type":"tags"},{"content":"In this article, we will discuss validating Kubernetes resource definitions using the tool kubeval.\nBefore we begin, it\u0026rsquo;s important to note that kubeval is no longer maintained, and the author advises migrating to kubeconform. We will explore kubeconform in the next article.\nIntroduction # Validating Kubernetes resources is very useful for detecting non-compliant resources before applying them to a cluster. The most common use case is to perform validation during CI to validate (or invalidate) the quality of a pull request.\nIn this specific case, a cluster is generally not available, and validation must therefore be performed offline. The tool must be able to function autonomously and validate (or invalidate) the supplied manifests.\nKubernetes-side Validation # Validation within a Kubernetes cluster is performed differently depending on the types of resources used.\nFor native Kubernetes resources (ConfigMap, Deployment, Pod, etc.), validation is done by dedicated code. The validation logic is therefore hard to leverage as it is not expressed in a declarative manner.\nNext comes the case of custom resources (described using CRDs). These types of resources are not native and must adhere to a more or less precise schema. This schema is used by Kubernetes to validate a resource submitted to it.\nFinally, it is also possible to enrich the validation logic by adding webhooks invoked by the Kubernetes API server at the time of resource admission.\nResource Schemas # The schemas for native or custom resources are expressed using OpenAPI v2 (or v3 for more recent versions of Kubernetes). These schemas are more or less precise in the case of native resources, with the validation logic being implemented in dedicated code.\nThese schemas are used by most resource validation tools to validate (or not) the proposed resources.\nValidation Webhooks # Until recently, validation webhooks were standard services, internal or external to the cluster, invoked by the API server during the admission of a resource.\nThese webhooks could implement a completely customized logic, and it is generally excluded that a validation tool could reproduce the logic embedded in these different services.\nRecently, the introduction of CEL and validation policies have allowed programming the behavior of the API server in a declarative manner. Validation tools are theoretically able to apply the same validation policies as the API server.\nInstalling kubeval # The kubeval project documentation suggests installing the binary directly or using a package manager, or using it through a Docker image.\nFor this article, we will use the Docker image, as installation with brew failed since the project is no longer maintained \u0026#x1f937;\nValidation with kubeval # First, let\u0026rsquo;s note that kubeval does not support validation of CRDs. The recommended workaround is to exclude custom (unknown) resources.\nLet\u0026rsquo;s try to validate the simple (yet invalid) manifest below:\napiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: nginx-ds spec: # no replicas field exist in DaemonSet resource replicas: 2 template: spec: containers: - image: nginx name: nginx If we run kubeval without any option the DaemonSet will be considered valid:\ndocker run -it -v ${PWD}:/work garethr/kubeval work/manifests.yaml PASS - work/manifests.yaml contains a valid DaemonSet (nginx-ds) To make kubeval detect additional fields, we need to specify --strict:\ndocker run -it -v ${PWD}:/work garethr/kubeval work/manifests.yaml --strict WARN - work/manifests.yaml contains an invalid DaemonSet (nginx-ds) - replicas: Additional property replicas is not allowed Target Kubernetes Version # In the case of native resources, the target version of the cluster is important, as native resources evolve regularly with the appearance of new properties, new API versions, some API versions are deprecated and then removed, etc.\nTherefore, a resource considered valid given one version of Kubernetes may be considered invalid with an older version of Kubernetes.\nIt is possible to target a specific version of Kubernetes using the -v argument. Unfortunately, the tool relies on schemas published on the website https://kubernetesjsonschema.dev which stopped at version 1.18 of Kubernetes.\nConclusion # It seems somewhat irrelevant to push the study of kubeval much further. It is clear that the tool is outdated, is no longer maintained, is at least 10 versions of Kubernetes behind, and has therefore clearly become obsolete.\nIt remains, however, the first tool to have popularized offline validation of Kubernetes resources and it deserved to be mentioned.\nWe will see other more modern tools in future articles. Stay tuned for the next episode!\n","date":"May 5, 2024","externalUrl":null,"permalink":"/website/.pr/99/articles/k8s-resources-validation/kubeval-review/","section":"Articles","summary":"In this article, we will discuss validating Kubernetes resource definitions using the tool kubeval.\nBefore we begin, it\u0026rsquo;s important to note that kubeval is no longer maintained, and the author advises migrating to kubeconform. We will explore kubeconform in the next article.\nIntroduction # Validating Kubernetes resources is very useful for detecting non-compliant resources before applying them to a cluster. The most common use case is to perform validation during CI to validate (or invalidate) the quality of a pull request.","title":"Validating Kubernetes resources offline - kubeval review","type":"articles"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/cilium/","section":"Tags","summary":"","title":"Cilium","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/kind/","section":"Tags","summary":"","title":"KinD","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/load-balancer/","section":"Tags","summary":"","title":"Load Balancer","type":"tags"},{"content":"","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/99/tags/macos/","section":"Tags","summary":"","title":"MacOS","type":"tags"},{"content":"Kubernetes in Docker (KinD) offers a lightweight and efficient way to run Kubernetes clusters for development and testing purposes. However, setting up KinD with load balancing option requires specific networking configurations. In this article, we\u0026rsquo;ll explore the networking configuration of KinD on both Linux and MacOS, deep dive into load balancing options and discuss troubleshooting tactics.\nRequirements # Docker KinD Kubectl Ciliumctl Setting Up Kubernetes in Docker # To create a KinD cluster with two nodes, you can use the following configuration:\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: disableDefaultCNI: true kubeProxyMode: none nodes: - role: control-plane - role: worker Verify the node status\nkubectl get nodes NAME STATUS ROLES AGE VERSION kind-control-plane NotReady control-plane 31s v1.29.2 kind-worker NotReady \u0026lt;none\u0026gt; 10s v1.29.2 The nodes are in a \u0026lsquo;NotReady\u0026rsquo; state because the CNI installation is disabled in the KinD config file, where \u0026lsquo;disableDefaultCNI\u0026rsquo; is set to true. This means that the essential networking layer required for pod communication and cluster operation is not configured. Consequently, the kubelet reports a \u0026ldquo;NotReady\u0026rdquo; state as it cannot establish network connectivity. Without CNI, pods cannot be assigned IP addresses.\nIn this article, we are going to use Cilium CNI. Cilium offers advanced networking features, enhanced security, service mesh integration, scalability, and comprehensive observability features. In this article, we’re going to explore some of Cilium advanced networking capabilities.\nDeploying Cilium is straightforward, let\u0026rsquo;s simplify things by applying the following configuration:\ncilium install \\ --set kubeProxyReplacement=\u0026#34;strict\u0026#34; \\ --set routingMode=\u0026#34;native\u0026#34; \\ --set ipv4NativeRoutingCIDR=\u0026#34;10.244.0.0/16\u0026#34; \\ --set k8sServiceHost=\u0026#34;kind-control-plane\u0026#34; \\ --set k8sServicePort=6443 \\ --set l2announcements.enabled=true \\ --set l2announcements.leaseDuration=\u0026#34;3s\u0026#34; \\ --set l2announcements.leaseRenewDeadline=\u0026#34;1s\u0026#34; \\ --set l2announcements.leaseRetryPeriod=\u0026#34;500ms\u0026#34; \\ --set devices=\u0026#34;{eth0,net0}\u0026#34; \\ --set externalIPs.enabled=true \\ --set autoDirectNodeRoutes=true \\ --set operator.replicas=2 Once the configuration is applied, you can verify the status of the Cilium deployment by executing the command cilium status --wait. This command will display the live deployment status of various Cilium components. Afterwards, running kubectl get nodes will display the nodes in a ready state, confirming the successful setup of networking with Cilium.\nNow let\u0026rsquo;s explore network configuration inside the kubernetes cluster:\nkubectl cluster-info dump | grep -m 1 cluster-cidr \u0026#34;--cluster-cidr=10.244.0.0/16\u0026#34;, As you can see, the Kubernetes cluster is configured with a cluster CIDR range of 10.244.0.0/16. This CIDR range is used by cilium CNI for assigning IP addresses to pods within the cluster.\nThe subnets assigned to each node are:\nkubectl get nodes -o jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.spec.podCIDR}{\u0026#34;\\n\u0026#34;}\u0026#39; kind-control-plane\t10.244.0.0/24 kind-worker\t10.244.1.0/24 Every node is informed about the IP addresses of all pods on every other node, and corresponding routes are added to the Linux kernel routing table of each node. This config is clearly visible when accessing a node within the cluster. You can do this by running docker exec -it kind-worker bash then display the routing table of the node.\nroot@kind-worker:/# ip route default via 172.18.0.1 dev eth0 10.244.0.0/24 via 172.18.0.3 dev eth0 proto kernel 10.244.1.0/24 via 10.244.1.228 dev cilium_host proto kernel src 10.244.1.228 10.244.1.228 dev cilium_host proto kernel scope link 172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2 default via 172.18.0.1 dev eth0: Any traffic that doesn\u0026rsquo;t match a more specific route in the table will be sent through the gateway with the IP address 172.18.0.1 and the outgoing interface eth0.\n10.244.0.0/24 via 172.18.0.3 dev eth0 proto kernel: Route for the subnet 10.244.0.0/24 (kind-control-plan) should be routed via the gateway with the IP address 172.18.0.3 and the outgoing interface eth0.\n10.244.1.0/24 via 10.244.1.228 dev cilium_host proto kernel src 10.244.1.228: Route for the subnet 10.244.1.0/24 (kind-worker) should be sent via the gateway with the IP address 10.244.1.228 via the interface cilium_host. Additionally, it specifies that the source IP address for this traffic should be 10.244.1.228.\n10.244.1.228 dev cilium_host proto kernel scope link: Network device cilium_host with the IP address 10.244.1.228 is directly connected the local network in the container.\nCilium\u0026rsquo;s Networking options # Cilium represents a modern solution for networking and security in Kubernetes, surpassing the capabilities of traditional components like kube-proxy (disabled in cilium installation kubeProxyReplacement=\u0026quot;strict\u0026quot;). While kube-proxy focuses on basic networking tasks such as service discovery and load balancing, Cilium extends its functionality with advanced networking, security, and observability features.\nIn terms of load balancing, kube-proxy operates at the network layer (L3/L4) and provides basic load balancing using iptables or IPVS. In contrast, Cilium can handle L3/L4 load balancing and offers more sophisticated techniques. We\u0026rsquo;ll delve into these techniques later in this article.\nCilium offers two network configurations: encapsulation and direct routing (native routingMode=\u0026quot;native\u0026quot;), each suited to different environments and requirements. Encapsulation works well in cloud environments or situations with overlapping networks while native routing is the best option in on-premises setups or dedicated cloud environments where performance optimization is crucial.\nFor further details on this topic, refer to the Cilium documentation: Cilium Routing Concepts.\nNetwork Configuration of KinD Cluster # The architecture of the KinD cluster leverages Docker\u0026rsquo;s networking features alongside standard Kubernetes components. Within a Kind cluster, every Kubernetes node is a Docker container. These containers operate within the same network namespace, facilitating communication via the Docker bridge network. KinD establishes a unique bridge network for each cluster for communication between Kubernetes nodes. Let\u0026rsquo;s explore docker networks.\nmacbook-pro % docker network list NETWORK ID NAME DRIVER SCOPE 8fd3d395c77e bridge bridge local 457bca38a85e host host local 53ced5328608 kind bridge local c96ad8e9a9bb none null local When examining the network setup, you\u0026rsquo;ll likely identify two bridge networks: bridge and kind. bridge is a system-wide bridge network managed by Docker, used for all Docker containers on the host machine. In contrast, kind is specific to each Kind cluster and used exclusively by Kubernetes nodes within that cluster.\nmacbook-pro % docker inspect kind [ { \u0026#34;Name\u0026#34;: \u0026#34;kind\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;53ced532860890f57acb0f315561d6af24550e0f052f886f3361bcc0ca94733f\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2024-03-12T16:24:53.318676142Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: true, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; }, { \u0026#34;Subnet\u0026#34;: \u0026#34;fc00:f853:ccd:e793::/64\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;fc00:f853:ccd:e793::1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;006079fd44c64434e818da824356ad71fd80ad05935f88777d1d2d906554ddf5\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;kind-control-plane\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;2da48bb2bbeaefdd98e4749777d7401c84d015e076a4b555dc1686ee1014d63d\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.3/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;fc00:f853:ccd:e793::3/64\u0026#34; }, \u0026#34;6731d2e4298603aa240dab421be84ae0beb1e5c81f27ea59c4097c4844462a82\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;kind-worker\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;d3f13cdea8f66fe3c29c2ff159828d977ef88cf7f2974944c11a3b16e18923d9\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:12:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;fc00:f853:ccd:e793::2/64\u0026#34; } }, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.bridge.enable_ip_masquerade\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;Labels\u0026#34;: {} } ] A Linux bridge behaves like a network switch. It forwards packets between interfaces that are connected to it. In the container world, The bridge functions as a virtual networking interface, interconnecting containers (Linux network namespaces) via virtual Ethernet pairs (veth pairs). Each container is configured with its own veth pair, with one end connected to the bridge network. This arrangement enables communication between containers internally and with the host system.\nLinux Namespaces play a pivotal role in containerization by providing isolated environments for individual containers. Each Docker container is encapsulated within its own namespace, ensuring it has its distinct IP address, routing table, and network configuration. This isolation mechanism prevents interference between containers and ensures the host system\u0026rsquo;s integrity. (please check Diving deep into Container Networking: An Exploration of Linux Network Namespace for more details)\nNow, let\u0026rsquo;s attempt to ping one of the node IPs. Remember, in a KinD cluster, nodes are Docker containers. If you\u0026rsquo;re using a Linux-based OS, the ping will be successful. However, if you\u0026rsquo;re on macOS, you\u0026rsquo;ll observe:\nmacbook-pro % ping 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 The ping is timing out!\nContainer\u0026rsquo;s Networking on MacOS # In macOS, Docker-for-Mac doesn\u0026rsquo;t directly expose container networks on the host system. Instead, it operates by running a Linux VM in the background and launches containers within that VM. It enables connections to containers via port binding (L4) and doesn\u0026rsquo;t support connections by IP address (L3).\nLet\u0026rsquo;s explore the routing table in the MacOS host:\nmacbook-pro % netstat -rn Routing tables Internet: Destination Gateway Flags Netif Expire default 192.168.1.1 UGScg en0 default link#17 UCSIg bridge100 ! 10.33.33.2 10.33.33.1 UH utun0 127 127.0.0.1 UCS lo0 127.0.0.1 127.0.0.1 UH lo0 169.254 link#6 UCS en0 ! 192.168.1 link#6 UCS en0 ! 192.168.1.1/32 link#6 UCS en0 ! 192.168.1.1 84:90🅰️3e:a2:4 UHLWIir en0 1126 192.168.1.10/32 link#6 UCS en0 ! 192.168.1.11 80:c:f9:6b:68:84 UHLWI en0 1100 192.168.64 link#17 UC bridge100 ! 192.168.64.1 16.7d.da.9a.15.64 UHLWI lo0 192.168.64.3 6.c5.4b.b4.32.aa UHLWIi bridge100 248 224.0.0/4 link#6 UmCS en0 ! 224.0.0.251 1:0:5e:0:0:fb UHmLWI en0 224.0.0.251 1:0:5e:0:0:fb UHmLWIg bridge100 239.255.255.250 1:0:5e:7f:ff:fa UHmLWI en0 255.255.255.255/32 link#6 UCS en0 ! As evident, there\u0026rsquo;s no network configuration to access the Docker network within the Docker Desktop VM. By contrast, comparing this with the Linux host\u0026rsquo;s ip route output reveals significant configuration differences.\nroot@ubuntu # ip route default via 192.168.64.1 dev enp0s1 proto dhcp src 192.168.64.3 metric 100 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.18.0.0/16 dev br-38055463db3c proto kernel scope link src 172.18.0.1 192.168.64.0/24 dev enp0s1 proto kernel scope link src 192.168.64.3 metric 100 192.168.64.1 dev enp0s1 proto dhcp scope link src 192.168.64.3 metric 100 Here, we can see a route to the container\u0026rsquo;s subnet via the network interface br-38055463db3c. The 172.18.0.0/16 dev br-38055463db3c proto kernel scope link src 172.18.0.1 line in the routing table indicates that the IP address range 172.18.0.0/16 is associated with the network device br-38055463db3c, which is the bridge interface.\nNow let’s list the network interfaces in the Linux host:\nroot@ubuntu # ip link … 4: br-38055463db3c: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c4:e4:5f:7e brd ff:ff:ff:ff:ff:ff 6: vethf413b4a@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br-38055463db3c state UP mode DEFAULT group default link/ether 0a:86:ac:2a:fe:a5 brd ff:ff:ff:ff:ff:ff link-netnsid 1 8: vethb9242a9@if7: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br-38055463db3c state UP mode DEFAULT group default link/ether a2:11:09:57:4f:26 brd ff:ff:ff:ff:ff:ff link-netnsid 0 … Let’s break down the configuration:\nbr-38055463db3c: The bridge interface.\nvethf413b4a@if5: This is a virtual Ethernet interface (veth) named \u0026ldquo;vethf413b4a\u0026rdquo; It is paired with another veth interface in a network namespace (container). It is connected to the bridge interface \u0026ldquo;br-38055463db3c\u0026rdquo;, this represents the link between the container and bridge discussed previously.\nvethb9242a9@if7: Similar to the second interface, this is another veth interface named \u0026ldquo;vethb9242a9\u0026rdquo;. It is paired with another veth interface in a network namespace (container). Also connected to the same bridge interface \u0026ldquo;br-38055463db3c\u0026rdquo;.\nThis means the docker containers are linked to the host via the bridge network. You can observe the traffic going in/out of the containers by running tcpdump -i vethf413b4a and tcpdump -i vethb9242a9 from your local host.\nTo achieve similar connectivity on macOS host, we are going to use docker mac net connect. This tool establishes a basic network tunnel between macOS and the Docker Desktop Linux VM. docker-mac-net-connect creates a virtual network interface (utun), acting as the bridge between your Mac and the Docker Desktop Linux VM.\nbrew install chipmk/tap/docker-mac-net-connect sudo brew services start chipmk/tap/docker-mac-net-connect Check the routing table again.\nmacbook-pro % netstat -rn Routing tables Internet: Destination Gateway Flags Netif Expire default 192.168.1.1 UGScg en0 default link#17 UCSIg bridge100 ! 10.33.33.2 10.33.33.1 UH utun0 127 127.0.0.1 UCS lo0 127.0.0.1 127.0.0.1 UH lo0 169.254 link#6 UCS en0 ! 172.17 utun0 USc utun0 172.18 utun0 USc utun0 192.168.1 link#6 UCS en0 ! 192.168.1.1/32 link#6 UCS en0 ! 192.168.1.1 84:90🅰️3e:a2:4 UHLWIir en0 1126 192.168.1.10/32 link#6 UCS en0 ! 192.168.1.11 80:c:f9:6b:68:84 UHLWI en0 1100 192.168.64 link#17 UC bridge100 ! 192.168.64.1 16.7d.da.9a.15.64 UHLWI lo0 192.168.64.3 6.c5.4b.b4.32.aa UHLWIi bridge100 248 224.0.0/4 link#6 UmCS en0 ! 224.0.0.251 1:0:5e:0:0:fb UHmLWI en0 224.0.0.251 1:0:5e:0:0:fb UHmLWIg bridge100 239.255.255.250 1:0:5e:7f:ff:fa UHmLWI en0 255.255.255.255/32 link#6 UCS en0 ! Now, multiple routes have been added to the routing table. 172.18 utun0 USc utun0 indicates that to access the subnet 172.18/16 (the docker network), the traffic should go through the network interface utun0. This network interface is connected on the other side of the tunnel to the Docker VM.\nThe other end of the tunnel (docker VM) is configured by a one-time container with sufficient privileges to configure the Linux host’s network interfaces. The container creates the interface, then exits. Despite the container\u0026rsquo;s termination, the VM interface continues to function because it was created within the Linux host’s network namespace.\nWith these configurations in place, the ping command will function once again, indicating that we now have access to the Docker\u0026rsquo;s network from the macOS host.\nmacbook-pro % ping 172.18.0.2 PING 172.18.0.2 (172.18.0.2): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 ... Request timeout for icmp_seq 20 64 bytes from 172.18.0.2: icmp_seq=21 ttl=63 time=3.334 ms 64 bytes from 172.18.0.2: icmp_seq=22 ttl=63 time=1.118 ms 64 bytes from 172.18.0.2: icmp_seq=23 ttl=63 time=1.016 ms Implementing Load Balancer service in KinD cluster with Cilium # To enable north/south traffic in your home lab, deploying a load balancer is a straightforward option. Cilium provides a load balancer feature that allows for load balancing in bare-metal Kubernetes setups. Announcing the load balancer IP to the network is crucial for proper traffic routing, which can be achieved through BGP or L2 routing.\nBGP: BGP stands as a dynamic routing protocol widely utilized in operational Kubernetes environments to broadcast external IP addresses. While its setup may hide greater complexity, BGP brings scalability and resilience through dynamic routing.\nL2 Layer: Alternatively, announcing the LB IP at the L2 layer simplifies the configuration process but may compromise the flexibility and scalability offered by BGP. This method suits smaller, less intricate deployments where simplicity takes precedence.\nWithin our home lab setup, we\u0026rsquo;ll use Cilium\u0026rsquo;s advanced networking features; LB-IPAM alongside L2 announcements. LB-IPAM handles the assignment of IP addresses to services of type LoadBalancer, while L2 announcements ensure services become visible and accessible across the local area network (L2 network).\nNow let\u0026rsquo;s deploy a simple Kubernetes service of type load balancer:\napiVersion: v1 kind: Service metadata: name: sampleservice spec: type: LoadBalancer ports: - port: 80 selector: app: myapp --- apiVersion: apps/v1 kind: Deployment metadata: name: sampleapp spec: selector: matchLabels: app: myapp replicas: 1 template: metadata: labels: app: myapp spec: containers: - name: myserver image: nginx imagePullPolicy: IfNotPresent Check the status of the service:\nmacbook-pro % kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h30m sampleservice LoadBalancer 10.96.28.96 \u0026lt;pending\u0026gt; 80:31508/TCP 4h53m As evident, the service is created with a pending external IP. To initiate IP assignment for the service load balancer, we need to deploy an lb-ipam pool. This pool defines the IP range from which cilium can select an IP address for the service.\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumLoadBalancerIPPool metadata: name: \u0026#34;lb-pool-1\u0026#34; spec: cidrs: - cidr: \u0026#34;172.18.250.0/24\u0026#34; Now let\u0026rsquo;s check the service again:\nmacbook-pro % kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h32m sampleservice LoadBalancer 10.96.28.96 172.18.250.1 80:31508/TCP 4h56m The external IP has now been allocated to the service from the LB-IPAM pool. To implement advanced filtering on the pool, such as service selectors, please consult the documentation here.\nOnce the IP has been assigned, we should be able to broadcast it locally (to all other devices sharing the same physical network L2). To achieve this, we need to create a cilium announcement policy.\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumL2AnnouncementPolicy metadata: name: announcement-policy-1 spec: externalIPs: false loadBalancerIPs: true interfaces: - ^eth[0-9]+ nodeSelector: matchExpressions: - key: node-role.kubernetes.io/control-plane operator: DoesNotExist The \u0026ldquo;interfaces\u0026rdquo; field specifies the network interfaces over which the chosen services will be broadcast. This field is optional, if left unspecified, all interfaces will be utilized. It\u0026rsquo;s essential to note that L2 announcements will function only if the selected devices are included in the \u0026lsquo;devices\u0026rsquo; set specified in the field \u0026lsquo;devices=\u0026quot;{eth0,net0}\u0026quot;\u0026rsquo;.\nL2 announcement functions are based on the Address Resolution Protocol (ARP). ARP is a fundamental protocol in computer networks used to map IP addresses to MAC addresses. Here\u0026rsquo;s a breakdown of ARP\u0026rsquo;s operation when trying to access Kubernetes load balancer service “curl 172.18.250.1/” from the local computer:\nARP Process Begins: The local computer checks its ARP cache to see if it has the MAC address of the destination IP address (docker container). If not found, an ARP request process begins.\nARP Request Packet Creation: The local computer constructs an ARP request packet containing the IP address of the destination (the LoadBalancer IP) and its own MAC address. The destination MAC address in the ARP request packet is set to the broadcast MAC address (FF:FF:FF:FF:FF:FF) to ensure all devices on the local network receive it.\nARP Request Broadcast: The local computer sends the ARP request packet as a broadcast frame onto the local network segment (L2 network). All devices on the local network receive this ARP request.\nCilium Agent Responds: after receiving the ARP request by the Kubernetes nodes, the Cilium agent checks if it has the specified IP address. If found, it responds with an ARP reply packet containing the MAC address of the node (in this case, the Docker container\u0026rsquo;s IP).\nARP Reply Packet: The ARP reply packet is sent directly to the MAC address of the local computer that initiated the ARP request.\nARP Cache Update: The local computer receives the ARP reply packet, extracts the MAC address of the LoadBalancer, and updates its ARP cache with this mapping. This mapping is cached for future use to avoid sending ARP requests for the same IP address.\nThe ARP table contains only the most recent MAC address associated with an IP. As a result, only one node in the cluster can reply to requests for a specific IP address. To ensure this, each Cilium agent selects services for its node and participates in leader election using Kubernetes leases. Every service corresponds to a lease, and the lease holder is responsible for responding to requests on designated interfaces.\nkubectl get leases -n kube-system NAME HOLDER AGE … cilium-l2announce-default-sampleservice kind-worker 4m2s … Now notice that the lease cilium-l2announce-default-sampleservice has elected kind-worker as a leader, in this case the cilium agent in this node will help in the ARP resolution process. Let\u0026rsquo;s capture the ARP traffic arriving at the kind-worker node using tcpdump.\nCILIUM_POD=$(kubectl -n kube-system get pod -l k8s-app=cilium --field-selector spec.nodeName=kind-worker -o name) kubectl -n kube-system exec -ti $CILIUM_POD -- bash apt-get update \u0026amp;\u0026amp; DEBIAN_FRONTEND=noninteractive apt-get -y install tcpdump termshark tcpdump -i any arp -w arp.pcap Now, open another terminal and run curl command with the Load Balancer IP address.\ncurl 172.18.250.1/ You\u0026rsquo;ll notice that the curl request is successful. Go back now to the Cilium agent terminal and stop the running command. Next, open Termshark and load the ARP file to examine the results.\nTERM=xterm-256color termshark -r arp.pcap In both screenshots we can see the ARP request and reply details provide insights into how network devices communicate and resolve MAC addresses to IP addresses. Here\u0026rsquo;s an explanation of each component:\nARP Request (1st screenshot): Sender\u0026rsquo;s IP Address: The IP address of the device sending the ARP request. Sender\u0026rsquo;s MAC Address: The MAC address of the device sending the ARP request. Target IP Address: The IP address for which the MAC address is being requested. Target MAC Address: This field is typically set to all zeros in ARP requests since the sender is asking for the MAC address associated with a specific IP address. It\u0026rsquo;s often referred to as the \u0026ldquo;broadcast MAC address\u0026rdquo; (FF:FF:FF:FF:FF:FF).\nARP Reply (2nd screenshot): Sender\u0026rsquo;s IP Address: The IP address of the device sending the ARP reply. Sender\u0026rsquo;s MAC Address: The MAC address of the device sending the ARP reply. Target IP Address: The IP address for which the MAC address is being provided. Target MAC Address: The MAC address associated with the target IP address, as requested in the ARP request.\nConclusion # Setting up the networking and load balancing in a KinD cluster with Cilium involves careful configuration and troubleshooting. By understanding the underlying principles and employing the right tools and techniques, you can ensure smooth operation and optimal performance of your home lab environment.\n","date":"May 2, 2024","externalUrl":null,"permalink":"/website/.pr/99/articles/networking/setting-up-load-balancer-service-with-cilium-in-kind-cluster/","section":"Articles","summary":"Kubernetes in Docker (KinD) offers a lightweight and efficient way to run Kubernetes clusters for development and testing purposes. However, setting up KinD with load balancing option requires specific networking configurations. In this article, we\u0026rsquo;ll explore the networking configuration of KinD on both Linux and MacOS, deep dive into load balancing options and discuss troubleshooting tactics.\nRequirements # Docker KinD Kubectl Ciliumctl Setting Up Kubernetes in Docker # To create a KinD cluster with two nodes, you can use the following configuration:","title":"Setting up Load Balancer Service with Cilium in KinD Cluster","type":"articles"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/99/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/99/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/99/categories/dev/","section":"Categories","summary":"","title":"Dev","type":"categories"},{"content":"","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/99/categories/frontend/","section":"Categories","summary":"","title":"Frontend","type":"categories"},{"content":"The first version of the homepage is up \u0026#x1f389;!\nCurrent tools # mkdocs mkdocs-material theme took a lot of inspiration from the blowfish Hugo theme Mixed feeling # While this worked reasonably well for the homepage, my feeling is that it required a lot of custom code and I spent way too much time writing HTML and CSS. This is probably not what we want to do when adding content, but instead, work with Markdown as much as possible.\nAt this point, I wonder if Hugo would have been a better choice, mkdocs is heavily focused on documentation websites, which is not the purpose of what we are trying to build \u0026#x1f914;\n","date":"April 26, 2024","externalUrl":null,"permalink":"/website/.pr/99/blog/posts/frontend-adventures/level-1/","section":"Blog","summary":"The first version of the homepage is up \u0026#x1f389;!\nCurrent tools # mkdocs mkdocs-material theme took a lot of inspiration from the blowfish Hugo theme Mixed feeling # While this worked reasonably well for the homepage, my feeling is that it required a lot of custom code and I spent way too much time writing HTML and CSS. This is probably not what we want to do when adding content, but instead, work with Markdown as much as possible.","title":"Frontend adventures - Level 1","type":"blog"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/99/podcasts/","section":"Podcasts","summary":"","title":"Podcasts","type":"podcasts"},{"content":"","date":"January 1, 1","externalUrl":null,"permalink":"/website/.pr/99/videos/","section":"Videos","summary":"","title":"Videos","type":"videos"}]